{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph Networks\n",
    "\n",
    "\n",
    "Till now, we have been focused on the *static* graphs where the graph structures and the node features are *fixed* over time. However, there are domains where the graph changes over time. \n",
    "\n",
    "Temporal graphs can be divided in two categories:\n",
    "- **Static graphs with temporal signals:** The underlying graph structure does not change over time, but features and labels evolve over time.\n",
    "        <center><img src=\"images/static_structure_dynamic_features.png\" width=\"500\"></center>\n",
    "    \n",
    "A main example is for traffic forcasting where graphs are based on traffic sensor data (e.g. the PeMS dataset) where each sensor is a node, and the edges are road connections. The geographical distribution of sensors in PeMS is shown below:\n",
    "        \n",
    "<center><img src=\"images/pems.ppm\" width=\"400\"></center>\n",
    "\n",
    "- **Dynamic graphs with temporal signals:** The topology of the graph (the presence of nodes and edges), features, and labels evolve over time.\n",
    "<center><img src=\"images/dynamic_structure_dynamic_features.png\" width=\"500\"></center>\n",
    "A main example is in a social network where new edges are added when people make new friends, existing edges are removed when people stop being friends, and node features change as people change their attributes, e.g., when they change their career assuming that career is one of the node features.\n",
    "<center><img src=\"images/Dynamic_Graphs.png\" width=\"500\"></center>\n",
    "\n",
    "> Note:\\\n",
    ">Dynamic graphs can be divided into *discrete-time* and *continuous-time* categories as well. \n",
    "\n",
    "- A discrete-time dynamic graph (DTDG) is a sequence $[G^{(1)}, G^{(2)},...,G^{(\\tau)}]$ of graph snapshots where each $G^{(t)} = \\left(V^{(t)},A^{(t)},X^{(t)}\\right)$ has vertices $V^{(t)}$, adjacency matrix $A^{(t)}$ and feature matrix $X^{(t)}$. DTDGs mainly appear in applications where data is captured at reguarly-spaced intervals.\n",
    "\n",
    "<center><img src=\"images/DTDG.png\" width=\"700\"></center>\n",
    "<center><small>Image from https://graph-neural-networks.github.io/static/file/chapter15.pdf</small></center> \n",
    "\n",
    "- A continuous-time dynamic graph (CTDG) is a pair $\\left(G^{(t_0)},O\\right)$ where ${G^{(t_0)}=\\left(V^{(t_0)},A^{(t_0)},X^{(t_0)}\\right)}$ is a static initial graph at initial state time $t_0$ and $O$ is a sequence of temporal observations/events. Each observation is a tuple of the form *(event, event type,timestamp)* where *event type* can be a node or edge addition, node or edge deletion, node feature update, etc. *event* represents the actual event that happened, and *timestamp* is the time at which the event occured:\n",
    "\n",
    "<center><img src=\"images/CTDG.png\" width=\"400\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2404.18211v1</small></center>\n",
    "\n",
    "We focus on DTDG in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining GNNs with sequence models\n",
    "\n",
    "DTDGs are made up of several snapshots arranged in order over time, which can be treated as sequential data. Temporal patterns in DTDGs are identified by looking at the relationships between these snapshots. Recurrent Neural Networks (RNNs) are often combined with GNNs to create dynamic models for DTDGs. These combinations are generally grouped into two types: stacked architectures and integrated architectures.\n",
    "\n",
    "- **Stacked dynamic GNNs:** The most straightforward way to model a discrete dynamic graph is to have a separate GNN handle each snapshot of the graph and feed the output of each GNN to a time series component, such as an RNN. This is illustrated in the following Figure: \n",
    "\n",
    "<center><img src=\"images/stacked_DTDG.png\" width=\"400\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2404.18211v1</small></center>\n",
    "\n",
    "One of most well-known approaches in this cateogry is Waterfall Dynamic-GCN. In this architectures, a GCN is stacked with an LSTM per node. More specifically, at first separate GCNs (with same parameters) handle each snapshot of the graph and next the output of each GNN is sequentially given to a LSTM. In fact, a separate LSTM is used per node (although the weights across the LSTMs are shared). The architecture is illustaretd in the following Figure:\n",
    "<center><img src=\"images/waterfall.png\" width=\"700\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2005.07496</small></center>\n",
    "\n",
    "The figure shows a network working on sequences of four snapshots of a graphs composed\n",
    "of five vertices. The first GCN layer acts as four copies of a regular GCN layer, each one working on a snapshot of the sequence of the graphs. The output of this first layer is processed by the LSTM layer that acts as five copies of a LSTM, each one working on a nodes of the graphs.\n",
    "The final fully-coonected (FC) layer produces the $C$-class probability vector for each nodes of every snapshot of the sequence. This layer, which produces the $C$-class probability vector for each node and for each instant of the sequence, can be seen as 5 x 4 copies of a FC layer.\n",
    "\n",
    "- **Integrated dynamic GNNs**: \n",
    "\n",
    "Integrated DGNNs are networks that combine GNNs and RNNs in one layer and thus combine modelling of the\n",
    "spatial and the temporal domain in that one layer.\n",
    "\n",
    "One major break-through approach in this category is <i>**EvolveGCN**</i>. <u>EvolveGCN applies Temporal neural networks such as RNNs to the *GCN parameters* themselves.</u> Note that GCN parameters are considered temporal and not the embeddings. In EvolveGCN, the GCN evolves over time to produce relevant temporal node embeddings. The following figure illustrates a high-level view of EvolveGCNâ€™s architecture to produce node embeddings for a static or dynamic graph with temporal signal:\n",
    "\n",
    "<center><img src=\"images/Evolvegcn.png\" width=\"700\"></center>\n",
    "<center><small>image from Labonne, Maxime. \"Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch\". Packt Publishing Ltd, 2023.</small></center>\n",
    "\n",
    "but how to use RNN-based models to update the parameters of GCN according to the timesteps? EvolveGCN proposed two similar architectures, which we introduce only one of them, manily EvolveGCN-H. The main idea is shown below:\n",
    "\n",
    "<center><img src=\"images/Evolvegcn-h.png\" width=\"700\"></center>\n",
    "\n",
    "EvolveGCN-H utilizes a Gated Recurrent Unit (GRU) in place of a standard RNN. The GRU, a simplified variant of the Long Short-Term Memory (LSTM) unit, offers similar performance with fewer parameters. In this architecture, the hidden state of the GRU corresponds to the weight matrix of the GCN. \n",
    "\n",
    "Let:\n",
    "- $H_t^{(l)}$ denote the node embeddings produced at the $l$-th layer and at the timestep $t$; (Note that $H_t^{(0)}=X$) \n",
    "- $W_{(t-1)}^{(l)}$ be the weight matrix for the GCN at layer $l$ and previous timestep $t-1$. \n",
    "\n",
    "At each time step $t$, the GRU takes the node embeddings from the previous layer, $H^{(l)}_t$, as input, and uses the GCN's weight matrix, $W^{(l)}_{t-1}$, as its hidden state. It then updates the $W$ matrix for layer $l$ at time $t$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "W_t^{(l)} = GRU(H_t^{(l)}, W_{t-1}^{(l)})\n",
    "\\end{equation*}\n",
    "\n",
    "The updated weight matrix is used to calculate the node embeddings for the $l+1$ layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "H^{(l+1)}_t = GCN(A_t, H_t^{(l)}, W_t^{(l)})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Temporal Graphs for Action based recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton joint order in UTD-MAD dataset:\n",
    "    head,\n",
    "    shoulder_center,\n",
    "    spine,\n",
    "    hip_center,\n",
    "    left_shoulder,\n",
    "    left_elbow,\n",
    "    left_wrist,\n",
    "    left_hand,\n",
    "    right_shoulder,\n",
    "    right_elbow,\n",
    "    right_wrist,\n",
    "    right_hand,\n",
    "    left_hip,\n",
    "    left_knee,\n",
    "    left_ankle,\n",
    "    left_foot,\n",
    "    right_hip,\n",
    "    right_knee,\n",
    "    right_ankle,\n",
    "    right_foot,\n",
    "\n",
    "Actions in Dataset\n",
    "    UTD-MHAD dataset consists of 27 different actions:\n",
    "    right arm swipe to the left,\n",
    "    right arm swipe to the right,\n",
    "    right hand wave,\n",
    "    two hand front clap,\n",
    "    right arm throw,\n",
    "    cross arms in the chest,\n",
    "    basketball shoot,\n",
    "    right hand draw x,\n",
    "    right hand draw circle (clockwise),\n",
    "    right hand draw circle (counter clockwise),\n",
    "    draw triangle,\n",
    "    bowling (right hand),\n",
    "    front boxing,\n",
    "    baseball swing from right,\n",
    "    tennis right hand forehand swing,\n",
    "    arm curl (two arms),\n",
    "    tennis serve,\n",
    "    two hand push,\n",
    "    right hand knock on door,\n",
    "    right hand catch an object,\n",
    "    right hand pick up and throw,\n",
    "    jogging in place,\n",
    "    walking in place,\n",
    "    sit to stand,\n",
    "    stand to sit,\n",
    "    forward lunge (left foot forward),\n",
    "    squat (two arms stretch out)\n",
    "\n",
    "Each skeleton data is a 20 x 3 x num_frame matrix. Each row of a skeleton frame corresponds to three spatial coordinates of a joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'gcn_weights' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 175\u001b[0m\n\u001b[0;32m    173\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_snapshots\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass graph snapshots for one action sequence\u001b[39;00m\n\u001b[0;32m    176\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([graph_snapshots[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39my])\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 88\u001b[0m, in \u001b[0;36mEvolveGCN_O.forward\u001b[1;34m(self, snapshots)\u001b[0m\n\u001b[0;32m     82\u001b[0m edge_index_t \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;66;03m# Edge index at time t\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Update the GCN weights using the GRU\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#for i in range(self.in_channels):\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#    self.gcn_weights[:,i] = self.gru(x_t, self.gcn_weights[:,i].unsqueeze(0))[1]\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn_weights\u001b[49m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x_t,(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_weights,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_t))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Assign the updated weights to the GCN layer\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn\u001b[38;5;241m.\u001b[39mlin\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_weights)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1708\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1708\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1709\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1710\u001b[0m                         )\n\u001b[0;32m   1711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'gcn_weights' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_max_pool\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTMMatrixCell(nn.Module):\n",
    "    def __init__(self, input_size1,input_size2, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size1 = input_size1\n",
    "        self.input_size2 = input_size2\n",
    "\n",
    "        # Input gate components\n",
    "        self.W_ii = nn.Parameter(torch.Tensor(hidden_size, input_size1))\n",
    "        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size, input_size2))\n",
    "\n",
    "        # Forget gate components\n",
    "        self.W_if = nn.Parameter(torch.Tensor(hidden_size, input_size1))\n",
    "        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size, input_size2))\n",
    "\n",
    "        # Cell gate components\n",
    "        self.W_ig = nn.Parameter(torch.Tensor(hidden_size,input_size1))\n",
    "        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_size, input_size2))\n",
    "\n",
    "        # Output gate components\n",
    "        self.W_io = nn.Parameter(torch.Tensor(hidden_size, input_size1))\n",
    "        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size, input_size2))\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        i_t = torch.sigmoid(self.W_ii @ x +  self.W_hi @ h_prev + self.b_i)\n",
    "        f_t = torch.sigmoid(self.W_if @ x + self.W_hf @ h_prev + self.b_f)\n",
    "        g_t = torch.tanh(self.W_ig @ x + self.W_hg @ h_prev + self.b_g)\n",
    "        o_t = torch.sigmoid(self.W_io @ x + self.W_ho @ h_prev + self.b_o)\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "class EvolveGCN_O(nn.Module):\n",
    "    def __init__(self, in_channels1, in_channels2, hidden_channels, out_channels):\n",
    "        super(EvolveGCN_O, self).__init__()\n",
    "        self.hidden_channels, self.in_channels1, self.in_channels2 = hidden_channels, in_channels1, in_channels2\n",
    "        # GCN layer (used for message passing)\n",
    "        self.gcn = GCNConv(in_channels2, hidden_channels)\n",
    "\n",
    "        # GRU to evolve the GCN weight matrix\n",
    "        #self.gru = nn.GRU(in_channels, hidden_channels, batch_first=True)\n",
    "        self.lstm = LSTMMatrixCell(in_channels1, in_channels2, hidden_channels)\n",
    "        # Linear layer for final output\n",
    "        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # Initialize the GCN weight matrix that will evolve over time\n",
    "        self.gcn_weights = torch.zeros(hidden_channels, in_channels2)\n",
    "        self.c_t = torch.zeros(hidden_channels,in_channels2)\n",
    "    def forward(self, snapshots):\n",
    "        for graph in snapshots:\n",
    "            x_t = graph.x      # Node features at time t\n",
    "            edge_index_t = graph.edge_index # Edge index at time t\n",
    "\n",
    "            # Update the GCN weights using the GRU\n",
    "\n",
    "            #for i in range(self.in_channels):\n",
    "            #    self.gcn_weights[:,i] = self.gru(x_t, self.gcn_weights[:,i].unsqueeze(0))[1]\n",
    "            self.gcn_weights,self.c_t = self.lstm(x_t,(self.gcn_weights,self.c_t))\n",
    "            # Assign the updated weights to the GCN layer\n",
    "            self.gcn.lin.weight = nn.Parameter(self.gcn_weights)\n",
    "\n",
    "            # Perform the GCN operation on the snapshot\n",
    "            h_t = self.gcn(x_t, edge_index_t)\n",
    "\n",
    "\n",
    "        # Final output for all snapshot\n",
    "        h_t= global_max_pool(h_t,torch.zeros(20, dtype=int))\n",
    "        h_t = self.fc1(h_t)\n",
    "        out_t = self.fc2(h_t)\n",
    "\n",
    "        return out_t\n",
    "\n",
    "# Example Training Loop\n",
    "model = EvolveGCN_O(in_channels1=20, in_channels2=3, hidden_channels=64, out_channels=27)  # 27 action classes in UTD-MHAD\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for file in os.listdir(\"Skeleton/\"):\n",
    "    if file.endswith(\"mat\"):\n",
    "        if \"s2\" in file or \"s4\" in file or \"s6\" in file or \"s8\" in file:\n",
    "            train.append((loadmat(\"Skeleton/\"+file)['d_skel'], file.split(\"_\")[0]))\n",
    "        else:\n",
    "            test.append((loadmat(\"Skeleton/\"+file)['d_skel'],file.split(\"_\")[0]))\n",
    "\n",
    "# Skeleton structure (joints connections) for UTD-MHAD\n",
    "skeleton_edges = [\n",
    "    (0, 1),  # Spine base to spine mid\n",
    "    (1, 2),  # Spine mid to spine top\n",
    "    (2, 3),  # Spine top to neck\n",
    "    (3, 4),  # Neck to head\n",
    "\n",
    "    (3, 5),  # Neck to left shoulder\n",
    "    (5, 6),  # Left shoulder to left elbow\n",
    "    (6, 7),  # Left elbow to left wrist\n",
    "    (7, 8),  # Left wrist to left hand\n",
    "\n",
    "    (3, 9),  # Neck to right shoulder\n",
    "    (9, 10), # Right shoulder to right elbow\n",
    "    (10, 11), # Right elbow to right wrist\n",
    "    (11, 12), # Right wrist to right hand\n",
    "\n",
    "    (0, 13), # Spine base to left hip\n",
    "    (13, 14), # Left hip to left knee\n",
    "    (14, 15), # Left knee to left ankle\n",
    "    (15, 16), # Left ankle to left foot\n",
    "\n",
    "    (0, 17), # Spine base to right hip\n",
    "    (17, 18), # Right hip to right knee\n",
    "    (18, 19)  # Right knee to right ankle\n",
    "]\n",
    "\n",
    "\n",
    "#Each skeleton data is a 20 x 3 x num_frame matrix.\n",
    "def create_graphs_from_tuple(in_tuple):\n",
    "    snapshots = []\n",
    "    action = int(in_tuple[1].split('a')[1]) - 1\n",
    "    frames = in_tuple[0]\n",
    "    for frame_num in range(frames.shape[2]):\n",
    "        joint_positions = frames[:,:,frame_num]\n",
    "        # Convert joint positions to tensor (nodes)\n",
    "        node_features = torch.tensor(joint_positions, dtype=torch.float).squeeze()\n",
    "\n",
    "        # Convert edge list to tensor\n",
    "        edge_index = torch.tensor(skeleton_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # Create PyG Data object\n",
    "        snapshots.append(Data(x=node_features, edge_index=edge_index, y=action))\n",
    "    return snapshots\n",
    "\n",
    "\n",
    "train_graph_snapshots = [create_graphs_from_tuple(seq) for seq in train]\n",
    "shuffle(train_graph_snapshots)\n",
    "test_graph_snapshots = [create_graphs_from_tuple(seq) for seq in test]\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for index,graph_snapshots in enumerate(train_graph_snapshots):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        out = model(graph_snapshots)  # Pass graph snapshots for one action sequence\n",
    "        labels = torch.as_tensor([graph_snapshots[0].y])\n",
    "        # Compute loss\n",
    "        loss = criterion(out, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}: graph # {index}, Loss: {loss.item()}')\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.array(losses).mean()}')\n",
    "    train_loss.append(np.array(losses).mean())\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for graph_snapshots in test_graph_snapshots:\n",
    "        # Forward pass through the model\n",
    "        out = model(graph_snapshots)  # Pass graph snapshots for one action sequence\n",
    "        label = graph_snapshots[0].y\n",
    "        # Compute loss\n",
    "        pred = torch.argmax(out)\n",
    "        if pred.item() == label:\n",
    "            correct +=1\n",
    "        total +=1\n",
    "    print(f'Test accuracy: {correct/total}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
