{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph Networks\n",
    "\n",
    "\n",
    "Till now, we have been focused on the *static* graphs where the graph structures and the node features are *fixed* over time. However, there are domains where the graph changes over time. \n",
    "\n",
    "Temporal graphs can be divided in two categories:\n",
    "- **Static graphs with temporal signals:** The underlying graph structure does not change over time, but features and labels evolve over time.\n",
    "        <center><img src=\"images/static_structure_dynamic_features.png\" width=\"500\"></center>\n",
    "    \n",
    "A main example is for traffic forcasting where graphs are based on traffic sensor data (e.g. the PeMS dataset) where each sensor is a node, and the edges are road connections. The geographical distribution of sensors in PeMS is shown below:\n",
    "        \n",
    "<center><img src=\"images/pems.ppm\" width=\"400\"></center>\n",
    "\n",
    "- **Dynamic graphs with temporal signals:** The topology of the graph (the presence of nodes and edges), features, and labels evolve over time.\n",
    "<center><img src=\"images/dynamic_structure_dynamic_features.png\" width=\"500\"></center>\n",
    "A main example is in a social network where new edges are added when people make new friends, existing edges are removed when people stop being friends, and node features change as people change their attributes, e.g., when they change their career assuming that career is one of the node features.\n",
    "<center><img src=\"images/Dynamic_Graphs.png\" width=\"500\"></center>\n",
    "\n",
    "> Note:\\\n",
    ">Dynamic graphs can be divided into *discrete-time* and *continuous-time* categories as well. \n",
    "\n",
    "- A discrete-time dynamic graph (DTDG) is a sequence $[G^{(1)}, G^{(2)},...,G^{(\\tau)}]$ of graph snapshots where each $G^{(t)} = \\left(V^{(t)},A^{(t)},X^{(t)}\\right)$ has vertices $V^{(t)}$, adjacency matrix $A^{(t)}$ and feature matrix $X^{(t)}$. DTDGs mainly appear in applications where data is captured at reguarly-spaced intervals.\n",
    "\n",
    "<center><img src=\"images/DTDG.png\" width=\"700\"></center>\n",
    "<center><small>Image from https://graph-neural-networks.github.io/static/file/chapter15.pdf</small></center> \n",
    "\n",
    "- A continuous-time dynamic graph (CTDG) is a pair $\\left(G^{(t_0)},O\\right)$ where ${G^{(t_0)}=\\left(V^{(t_0)},A^{(t_0)},X^{(t_0)}\\right)}$ is a static initial graph at initial state time $t_0$ and $O$ is a sequence of temporal observations/events. Each observation is a tuple of the form *(event, event type,timestamp)* where *event type* can be a node or edge addition, node or edge deletion, node feature update, etc. *event* represents the actual event that happened, and *timestamp* is the time at which the event occured:\n",
    "\n",
    "<center><img src=\"images/CTDG.png\" width=\"400\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2404.18211v1</small></center>\n",
    "\n",
    "We focus on DTDG in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining GNNs with sequence models\n",
    "\n",
    "DTDGs are made up of several snapshots arranged in order over time, which can be treated as sequential data. Temporal patterns in DTDGs are identified by looking at the relationships between these snapshots. Recurrent Neural Networks (RNNs) are often combined with GNNs to create dynamic models for DTDGs. These combinations are generally grouped into two types: stacked architectures and integrated architectures.\n",
    "\n",
    "- **Stacked dynamic GNNs:** The most straightforward way to model a discrete dynamic graph is to have a separate GNN handle each snapshot of the graph and feed the output of each GNN to a time series component, such as an RNN. This is illustrated in the following Figure: \n",
    "\n",
    "<center><img src=\"images/stacked_DTDG.png\" width=\"400\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2404.18211v1</small></center>\n",
    "\n",
    "One of most well-known approaches in this cateogry is Waterfall Dynamic-GCN. In this architectures, a GCN is stacked with an LSTM per node. More specifically, at first separate GCNs (with same parameters) handle each snapshot of the graph and next the output of each GNN is sequentially given to a LSTM. In fact, a separate LSTM is used per node (although the weights across the LSTMs are shared). The architecture is illustaretd in the following Figure:\n",
    "<center><img src=\"images/waterfall.png\" width=\"700\"></center>\n",
    "<center><small>Image from https://arxiv.org/pdf/2005.07496</small></center>\n",
    "\n",
    "The figure shows a network working on sequences of four snapshots of a graphs composed\n",
    "of five vertices. The first GCN layer acts as four copies of a regular GCN layer, each one working on a snapshot of the sequence of the graphs. The output of this first layer is processed by the LSTM layer that acts as five copies of a LSTM, each one working on a nodes of the graphs.\n",
    "The final fully-coonected (FC) layer produces the $C$-class probability vector for each nodes of every snapshot of the sequence. This layer, which produces the $C$-class probability vector for each node and for each instant of the sequence, can be seen as 5 x 4 copies of a FC layer.\n",
    "\n",
    "- **Integrated dynamic GNNs**: \n",
    "\n",
    "Integrated DGNNs are networks that combine GNNs and RNNs in one layer and thus combine modelling of the\n",
    "spatial and the temporal domain in that one layer.\n",
    "\n",
    "One major break-through approach in this category is <i>**EvolveGCN**</i>. <u>EvolveGCN applies Temporal neural networks such as RNNs to the *GCN parameters* themselves.</u> Note that GCN parameters are considered temporal and not the embeddings. In EvolveGCN, the GCN evolves over time to produce relevant temporal node embeddings. The following figure illustrates a high-level view of EvolveGCN’s architecture to produce node embeddings for a static or dynamic graph with temporal signal:\n",
    "\n",
    "<center><img src=\"images/Evolvegcn.png\" width=\"700\"></center>\n",
    "<center><small>image from Labonne, Maxime. \"Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch\". Packt Publishing Ltd, 2023.</small></center>\n",
    "\n",
    "but how to use RNN-based models to update the parameters of GCN according to the timesteps? EvolveGCN proposed two similar architectures, which we introduce only one of them, manily EvolveGCN-H. The main idea is shown below:\n",
    "\n",
    "<center><img src=\"images/Evolvegcn-h.png\" width=\"700\"></center>\n",
    "\n",
    "EvolveGCN-H utilizes a Gated Recurrent Unit (GRU) in place of a standard RNN. The GRU, a simplified variant of the Long Short-Term Memory (LSTM) unit, offers similar performance with fewer parameters. In this architecture, the hidden state of the GRU corresponds to the weight matrix of the GCN. \n",
    "\n",
    "Let:\n",
    "- $H_t^{(l)}$ denote the node embeddings produced at the $l$-th layer and at the timestep $t$; (Note that $H_t^{(0)}=X$) \n",
    "- $W_{(t-1)}^{(l)}$ be the weight matrix for the GCN at layer $l$ and previous timestep $t-1$. \n",
    "\n",
    "At each time step $t$, the GRU takes the node embeddings from the previous layer, $H^{(l)}_t$, as input, and uses the GCN's weight matrix, $W^{(l)}_{t-1}$, as its hidden state. It then updates the $W$ matrix for layer $l$ at time $t$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "W_t^{(l)} = GRU(H_t^{(l)}, W_{t-1}^{(l)})\n",
    "\\end{equation*}\n",
    "\n",
    "The updated weight matrix is used to calculate the node embeddings for the $l+1$ layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "H^{(l+1)}_t = GCN(A_t, H_t^{(l)}, W_t^{(l)})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Temporal Graphs for Action based recognition in Pytorch\n",
    "\n",
    "\n",
    "Skeleton-based action recognition leverages human skeletal data to identify and classify actions or gestures. This approach is particularly effective because skeleton data provides a structured and abstract representation of body movements, capturing key joint positions and their interactions. \n",
    "\n",
    "<center><img src=\"images/skeleton_as_graph.png\" width=\"300\"></center>\n",
    "\n",
    "- Skeleton Data: The input consists of sequences of joint positions, typically represented as a set of coordinates for each joint in a human skeleton. Each joint's position is recorded over time, creating a sequence of frames that shows how the skeleton moves.\n",
    "- Graph Representation: In the graph-based approach, each joint is treated as a node in a graph, and edges represent the connections between joints (e.g., bones). This graph captures the spatial relationships and constraints between different parts of the body.\n",
    "- Spatio-Temporal Modeling:\n",
    "\n",
    "    - Spatial Analysis: To analyze the spatial relationships between joints, graph convolutional layers are used. These layers capture how the joints interact with each other within a single frame.\n",
    "    - Temporal Analysis: To understand how these interactions change over time, temporal convolutional layers process sequences of frames. This helps in learning the dynamic patterns of movement across time.\n",
    "\n",
    "Spatio-Temporal Graph Convolutional Networks (ST-GCNs) are a pioneering approach designed to handle data that involves both spatial and temporal dimensions, particularly suited for tasks like action recognition from skeleton data. \n",
    "In ST-GCNs, skeleton data is represented as a graph where joints (body parts) are nodes and their connections (bones) are edges. This graph structure captures the spatial relationships between different joints in a single frame.\n",
    "ST-GCNs are composed of several layers. In each layer, ST-GCNs use GCN to apply convolution operations on the graph structure, aggregating information from neighboring nodes (joints) to learn how each joint's position relates to others. This helps in capturing the spatial patterns of body movements. Since actions involve sequences of movements over time, ST-GCNs also include one-dimensional temporal convolutional layers. These layers process sequences of frames, allowing the model to learn how the spatial configuration of joints changes over time. Temporal convolutions help capture dynamic patterns and transitions in the motion data.\n",
    "\n",
    "In this section, we will develop a basic implementation of the ST-GCN model using PyTorch Geometric, focusing on skeleton-based action recognition with the UTD-MHAD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The skeleton joint order in UTD-MAD dataset: \\\n",
    "    head,\n",
    "    shoulder_center,\n",
    "    spine,\n",
    "    hip_center,\n",
    "    left_shoulder,\n",
    "    left_elbow,\n",
    "    left_wrist,\n",
    "    left_hand,\n",
    "    right_shoulder,\n",
    "    right_elbow,\n",
    "    right_wrist,\n",
    "    right_hand,\n",
    "    left_hip,\n",
    "    left_knee,\n",
    "    left_ankle,\n",
    "    left_foot,\n",
    "    right_hip,\n",
    "    right_knee,\n",
    "    right_ankle,\n",
    "    right_foot,\n",
    "\n",
    "> UTD-MHAD dataset consists of 27 different actions: \\\n",
    "    right arm swipe to the left,\n",
    "    right arm swipe to the right,\n",
    "    right hand wave,\n",
    "    two hand front clap,\n",
    "    right arm throw,\n",
    "    cross arms in the chest,\n",
    "    basketball shoot,\n",
    "    right hand draw x,\n",
    "    right hand draw circle (clockwise),\n",
    "    right hand draw circle (counter clockwise),\n",
    "    draw triangle,\n",
    "    bowling (right hand),\n",
    "    front boxing,\n",
    "    baseball swing from right,\n",
    "    tennis right hand forehand swing,\n",
    "    arm curl (two arms),\n",
    "    tennis serve,\n",
    "    two hand push,\n",
    "    right hand knock on door,\n",
    "    right hand catch an object,\n",
    "    right hand pick up and throw,\n",
    "    jogging in place,\n",
    "    walking in place,\n",
    "    sit to stand,\n",
    "    stand to sit,\n",
    "    forward lunge (left foot forward),\n",
    "    squat (two arms stretch out)\n",
    "\n",
    "Each skeleton data is a 20 x 3 x num_frame matrix. Each row of a skeleton frame corresponds to three spatial coordinates of a joint. \n",
    "\n",
    "The UTD-MAD dataset features a total of 8 subjects performing a variety of actions. It includes 27 distinct action categories, with each subject performing four takes (repetitions) of each action. Specifically, there are about 1,280 action sequences captured across all subjects and actions. This diversity in subjects and takes provides a comprehensive foundation for developing and evaluating action recognition algorithms.\n",
    "\n",
    "We use even subjects (S2, S4, S6, S8) to train, Odd Subjects (S1, S3, S5, S7) to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.7029674391413843, Train accuracy: 0.13488372093023257\n",
      "Test accuracy: 0.12296983758700696\n",
      "Epoch 2, Loss: 2.3595689116522323, Train accuracy: 0.27906976744186046\n",
      "Test accuracy: 0.271461716937355\n",
      "Epoch 3, Loss: 1.866786751151085, Train accuracy: 0.35348837209302325\n",
      "Test accuracy: 0.3874709976798144\n",
      "Epoch 4, Loss: 1.5716009542259366, Train accuracy: 0.4744186046511628\n",
      "Test accuracy: 0.4617169373549884\n",
      "Epoch 5, Loss: 1.2992200438604624, Train accuracy: 0.5441860465116279\n",
      "Test accuracy: 0.4895591647331787\n",
      "Epoch 6, Loss: 1.1048050865124868, Train accuracy: 0.6046511627906976\n",
      "Test accuracy: 0.5707656612529002\n",
      "Epoch 7, Loss: 0.9868400013664478, Train accuracy: 0.6372093023255814\n",
      "Test accuracy: 0.5893271461716937\n",
      "Epoch 8, Loss: 0.8508805212198599, Train accuracy: 0.6093023255813953\n",
      "Test accuracy: 0.6032482598607889\n",
      "Epoch 9, Loss: 0.737859922967889, Train accuracy: 0.6813953488372093\n",
      "Test accuracy: 0.6890951276102089\n",
      "Epoch 10, Loss: 0.659264627443596, Train accuracy: 0.6604651162790698\n",
      "Test accuracy: 0.617169373549884\n",
      "Epoch 11, Loss: 0.5591343767997852, Train accuracy: 0.6488372093023256\n",
      "Test accuracy: 0.654292343387471\n",
      "Epoch 12, Loss: 0.4555091519581173, Train accuracy: 0.6906976744186046\n",
      "Test accuracy: 0.6264501160092807\n",
      "Epoch 13, Loss: 0.4235284084808501, Train accuracy: 0.7093023255813954\n",
      "Test accuracy: 0.665893271461717\n",
      "Epoch 14, Loss: 0.3581945154065981, Train accuracy: 0.7581395348837209\n",
      "Test accuracy: 0.679814385150812\n",
      "Epoch 15, Loss: 0.29462154882151714, Train accuracy: 0.6976744186046512\n",
      "Test accuracy: 0.6774941995359629\n",
      "Epoch 16, Loss: 0.31307432444458394, Train accuracy: 0.7581395348837209\n",
      "Test accuracy: 0.7053364269141531\n",
      "Epoch 17, Loss: 0.21751269517818655, Train accuracy: 0.7627906976744186\n",
      "Test accuracy: 0.7146171693735499\n",
      "Epoch 18, Loss: 0.1751895161582937, Train accuracy: 0.7674418604651163\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 19, Loss: 0.21439575904492644, Train accuracy: 0.7441860465116279\n",
      "Test accuracy: 0.6983758700696056\n",
      "Epoch 20, Loss: 0.19011618621410423, Train accuracy: 0.7162790697674418\n",
      "Test accuracy: 0.6751740139211136\n",
      "Epoch 21, Loss: 0.15188835157762295, Train accuracy: 0.7976744186046512\n",
      "Test accuracy: 0.765661252900232\n",
      "Epoch 22, Loss: 0.1589197967977064, Train accuracy: 0.7906976744186046\n",
      "Test accuracy: 0.7146171693735499\n",
      "Epoch 23, Loss: 0.1552434012728279, Train accuracy: 0.7930232558139535\n",
      "Test accuracy: 0.728538283062645\n",
      "Epoch 24, Loss: 0.08984375642804601, Train accuracy: 0.7\n",
      "Test accuracy: 0.6774941995359629\n",
      "Epoch 25, Loss: 0.10989886472353609, Train accuracy: 0.8023255813953488\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 26, Loss: 0.10661129374308632, Train accuracy: 0.7674418604651163\n",
      "Test accuracy: 0.703016241299304\n",
      "Epoch 27, Loss: 0.09345332180942949, Train accuracy: 0.7093023255813954\n",
      "Test accuracy: 0.6705336426914154\n",
      "Epoch 28, Loss: 0.07620037987155723, Train accuracy: 0.7441860465116279\n",
      "Test accuracy: 0.7122969837587007\n",
      "Epoch 29, Loss: 0.10719726990617354, Train accuracy: 0.8069767441860465\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 30, Loss: 0.06886884597220932, Train accuracy: 0.8116279069767441\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 31, Loss: 0.047405017226939766, Train accuracy: 0.8046511627906977\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 32, Loss: 0.07194841966190858, Train accuracy: 0.7372093023255814\n",
      "Test accuracy: 0.703016241299304\n",
      "Epoch 33, Loss: 0.09457639185415204, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7540603248259861\n",
      "Epoch 34, Loss: 0.023442209086676463, Train accuracy: 0.7953488372093023\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 35, Loss: 0.0766316823869014, Train accuracy: 0.827906976744186\n",
      "Test accuracy: 0.7703016241299304\n",
      "Epoch 36, Loss: 0.08824514364580624, Train accuracy: 0.8023255813953488\n",
      "Test accuracy: 0.7517401392111369\n",
      "Epoch 37, Loss: 0.04950763437853524, Train accuracy: 0.8232558139534883\n",
      "Test accuracy: 0.7354988399071926\n",
      "Epoch 38, Loss: 0.053476314303422846, Train accuracy: 0.7488372093023256\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 39, Loss: 0.01785578391728135, Train accuracy: 0.7674418604651163\n",
      "Test accuracy: 0.7331786542923434\n",
      "Epoch 40, Loss: 0.019164769888073623, Train accuracy: 0.7511627906976744\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 41, Loss: 0.1646158604748035, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 42, Loss: 0.020020466107886013, Train accuracy: 0.827906976744186\n",
      "Test accuracy: 0.7494199535962877\n",
      "Epoch 43, Loss: 0.008058294720031901, Train accuracy: 0.8069767441860465\n",
      "Test accuracy: 0.7215777262180975\n",
      "Epoch 44, Loss: 0.017703247263846082, Train accuracy: 0.8488372093023255\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 45, Loss: 0.09619186671666011, Train accuracy: 0.7790697674418605\n",
      "Test accuracy: 0.7308584686774942\n",
      "Epoch 46, Loss: 0.0847316928453069, Train accuracy: 0.8162790697674419\n",
      "Test accuracy: 0.7238979118329466\n",
      "Epoch 47, Loss: 0.035350344181996736, Train accuracy: 0.8488372093023255\n",
      "Test accuracy: 0.7679814385150812\n",
      "Epoch 48, Loss: 0.040515152496574905, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 49, Loss: 0.024511022933714658, Train accuracy: 0.8232558139534883\n",
      "Test accuracy: 0.7354988399071926\n",
      "Epoch 50, Loss: 0.0037554789003149043, Train accuracy: 0.7883720930232558\n",
      "Test accuracy: 0.7378190255220418\n",
      "Epoch 51, Loss: 0.008230454794324714, Train accuracy: 0.7930232558139535\n",
      "Test accuracy: 0.7308584686774942\n",
      "Epoch 52, Loss: 0.00658382086086229, Train accuracy: 0.813953488372093\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 53, Loss: 0.09344571232447658, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.740139211136891\n",
      "Epoch 54, Loss: 0.039041108072550935, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7308584686774942\n",
      "Epoch 55, Loss: 0.0057194682150863, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.7192575406032483\n",
      "Epoch 56, Loss: 0.0019941896355309546, Train accuracy: 0.8302325581395349\n",
      "Test accuracy: 0.728538283062645\n",
      "Epoch 57, Loss: 0.005829633005665994, Train accuracy: 0.827906976744186\n",
      "Test accuracy: 0.7215777262180975\n",
      "Epoch 58, Loss: 0.002678868093422421, Train accuracy: 0.7930232558139535\n",
      "Test accuracy: 0.7006960556844548\n",
      "Epoch 59, Loss: 0.001952077267138232, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7494199535962877\n",
      "Epoch 60, Loss: 0.0020200409056728807, Train accuracy: 0.8069767441860465\n",
      "Test accuracy: 0.6937354988399071\n",
      "Epoch 61, Loss: 0.004284538619685621, Train accuracy: 0.7441860465116279\n",
      "Test accuracy: 0.6496519721577726\n",
      "Epoch 62, Loss: 0.25722785435080064, Train accuracy: 0.8302325581395349\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 63, Loss: 0.011337964015845218, Train accuracy: 0.8395348837209302\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 64, Loss: 0.004995932432282013, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 65, Loss: 0.04368652886017835, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.703016241299304\n",
      "Epoch 66, Loss: 0.03701604624127743, Train accuracy: 0.8302325581395349\n",
      "Test accuracy: 0.7819025522041764\n",
      "Epoch 67, Loss: 0.015497046729408432, Train accuracy: 0.8418604651162791\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 68, Loss: 0.01503850588830797, Train accuracy: 0.8418604651162791\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 69, Loss: 0.002835407112599333, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 70, Loss: 0.054403809743688296, Train accuracy: 0.8232558139534883\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 71, Loss: 0.005069769299916499, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 72, Loss: 0.0028918850432367977, Train accuracy: 0.8441860465116279\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 73, Loss: 0.0016401457039985172, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.7563805104408353\n",
      "Epoch 74, Loss: 0.0017021197768479648, Train accuracy: 0.8441860465116279\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 75, Loss: 0.05488447999403534, Train accuracy: 0.8488372093023255\n",
      "Test accuracy: 0.7424593967517401\n",
      "Epoch 76, Loss: 0.05943102882681203, Train accuracy: 0.7906976744186046\n",
      "Test accuracy: 0.7354988399071926\n",
      "Epoch 77, Loss: 0.004467069686244404, Train accuracy: 0.8\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 78, Loss: 0.01568516322761904, Train accuracy: 0.8604651162790697\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 79, Loss: 0.0026026295014358125, Train accuracy: 0.8\n",
      "Test accuracy: 0.7099767981438515\n",
      "Epoch 80, Loss: 0.0025241847409833737, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 81, Loss: 0.0014112376456428455, Train accuracy: 0.8186046511627907\n",
      "Test accuracy: 0.7308584686774942\n",
      "Epoch 82, Loss: 0.0014041119458956604, Train accuracy: 0.8441860465116279\n",
      "Test accuracy: 0.7795823665893271\n",
      "Epoch 83, Loss: 0.07358421943471796, Train accuracy: 0.8\n",
      "Test accuracy: 0.7215777262180975\n",
      "Epoch 84, Loss: 0.07972119982913069, Train accuracy: 0.7930232558139535\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 85, Loss: 0.004749290237082505, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7703016241299304\n",
      "Epoch 86, Loss: 0.0023299759214994486, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 87, Loss: 0.002164015767490675, Train accuracy: 0.8441860465116279\n",
      "Test accuracy: 0.765661252900232\n",
      "Epoch 88, Loss: 0.0025969852015115817, Train accuracy: 0.813953488372093\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 89, Loss: 0.0012690979561494644, Train accuracy: 0.8441860465116279\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 90, Loss: 0.002577253931954434, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.740139211136891\n",
      "Epoch 91, Loss: 0.0013883503117133559, Train accuracy: 0.8186046511627907\n",
      "Test accuracy: 0.740139211136891\n",
      "Epoch 92, Loss: 0.000728217875102214, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7703016241299304\n",
      "Epoch 93, Loss: 0.0006736033204981, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 94, Loss: 0.11707894586270638, Train accuracy: 0.8511627906976744\n",
      "Test accuracy: 0.7819025522041764\n",
      "Epoch 95, Loss: 0.005984733380856734, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7795823665893271\n",
      "Epoch 96, Loss: 0.005717620120598999, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7842227378190255\n",
      "Epoch 97, Loss: 0.00491102455294048, Train accuracy: 0.8511627906976744\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 98, Loss: 0.0011504149842205197, Train accuracy: 0.8395348837209302\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 99, Loss: 0.0012228884801639562, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 100, Loss: 0.000981731078853641, Train accuracy: 0.8395348837209302\n",
      "Test accuracy: 0.7981438515081206\n",
      "Epoch 101, Loss: 0.066439456805511, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 102, Loss: 0.08549623404988857, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 103, Loss: 0.01133249874063798, Train accuracy: 0.8232558139534883\n",
      "Test accuracy: 0.765661252900232\n",
      "Epoch 104, Loss: 0.0020044248511038665, Train accuracy: 0.8418604651162791\n",
      "Test accuracy: 0.7563805104408353\n",
      "Epoch 105, Loss: 0.0034190656177947648, Train accuracy: 0.8348837209302326\n",
      "Test accuracy: 0.7679814385150812\n",
      "Epoch 106, Loss: 0.0026473807904843763, Train accuracy: 0.8418604651162791\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 107, Loss: 0.05951226917265147, Train accuracy: 0.8186046511627907\n",
      "Test accuracy: 0.740139211136891\n",
      "Epoch 108, Loss: 0.030637205705423547, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 109, Loss: 0.0022396203160859164, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 110, Loss: 0.0021992831121530492, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.765661252900232\n",
      "Epoch 111, Loss: 0.0010884264918218254, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.777262180974478\n",
      "Epoch 112, Loss: 0.0032961685953661534, Train accuracy: 0.8395348837209302\n",
      "Test accuracy: 0.7865429234338747\n",
      "Epoch 113, Loss: 0.05538214553813669, Train accuracy: 0.813953488372093\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 114, Loss: 0.004867710104122571, Train accuracy: 0.8\n",
      "Test accuracy: 0.7122969837587007\n",
      "Epoch 115, Loss: 0.039584971573396244, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 116, Loss: 0.00192045907592069, Train accuracy: 0.8325581395348837\n",
      "Test accuracy: 0.7540603248259861\n",
      "Epoch 117, Loss: 0.0008995579709989484, Train accuracy: 0.8488372093023255\n",
      "Test accuracy: 0.7587006960556845\n",
      "Epoch 118, Loss: 0.0007490453525737814, Train accuracy: 0.8465116279069768\n",
      "Test accuracy: 0.7679814385150812\n",
      "Epoch 119, Loss: 0.0004933873567905711, Train accuracy: 0.8372093023255814\n",
      "Test accuracy: 0.7424593967517401\n",
      "Epoch 120, Loss: 0.0031299693616003176, Train accuracy: 0.8116279069767441\n",
      "Test accuracy: 0.7262180974477959\n",
      "Epoch 121, Loss: 0.001502033925020095, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7447795823665894\n",
      "Epoch 122, Loss: 0.049739484036967124, Train accuracy: 0.7906976744186046\n",
      "Test accuracy: 0.6728538283062645\n",
      "Epoch 123, Loss: 0.06245830431344888, Train accuracy: 0.8558139534883721\n",
      "Test accuracy: 0.7633410672853829\n",
      "Epoch 124, Loss: 0.0016656887539605962, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7308584686774942\n",
      "Epoch 125, Loss: 0.025010429791985346, Train accuracy: 0.8813953488372093\n",
      "Test accuracy: 0.7958236658932715\n",
      "Epoch 126, Loss: 0.0018454944569435243, Train accuracy: 0.858139534883721\n",
      "Test accuracy: 0.7703016241299304\n",
      "Epoch 127, Loss: 0.007538798812674908, Train accuracy: 0.813953488372093\n",
      "Test accuracy: 0.765661252900232\n",
      "Epoch 128, Loss: 0.0007541256569656068, Train accuracy: 0.8069767441860465\n",
      "Test accuracy: 0.7424593967517401\n",
      "Epoch 129, Loss: 0.0010019599461331618, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 130, Loss: 0.0007969609980217707, Train accuracy: 0.8209302325581396\n",
      "Test accuracy: 0.7331786542923434\n",
      "Epoch 131, Loss: 0.0009076029171528466, Train accuracy: 0.8162790697674419\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 132, Loss: 0.0011058898742266868, Train accuracy: 0.8255813953488372\n",
      "Test accuracy: 0.7679814385150812\n",
      "Epoch 133, Loss: 0.0005987344939004409, Train accuracy: 0.8186046511627907\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 134, Loss: 0.000394804846221055, Train accuracy: 0.8465116279069768\n",
      "Test accuracy: 0.7795823665893271\n",
      "Epoch 135, Loss: 0.10952138536715887, Train accuracy: 0.786046511627907\n",
      "Test accuracy: 0.7099767981438515\n",
      "Epoch 136, Loss: 0.024871806219081374, Train accuracy: 0.8465116279069768\n",
      "Test accuracy: 0.7540603248259861\n",
      "Epoch 137, Loss: 0.0018548027292407537, Train accuracy: 0.8232558139534883\n",
      "Test accuracy: 0.7470997679814385\n",
      "Epoch 138, Loss: 0.008336645446116165, Train accuracy: 0.858139534883721\n",
      "Test accuracy: 0.7610208816705336\n",
      "Epoch 139, Loss: 0.0019294353193574846, Train accuracy: 0.8674418604651163\n",
      "Test accuracy: 0.7540603248259861\n",
      "Epoch 140, Loss: 0.0012088971800609627, Train accuracy: 0.8558139534883721\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 141, Loss: 0.000876252946214061, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 142, Loss: 0.0005516025990310424, Train accuracy: 0.8813953488372093\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 143, Loss: 0.0004829728864618596, Train accuracy: 0.872093023255814\n",
      "Test accuracy: 0.7935034802784223\n",
      "Epoch 144, Loss: 0.0006241038114382696, Train accuracy: 0.8418604651162791\n",
      "Test accuracy: 0.7169373549883991\n",
      "Epoch 145, Loss: 0.0007108194166967177, Train accuracy: 0.8558139534883721\n",
      "Test accuracy: 0.7726218097447796\n",
      "Epoch 146, Loss: 0.11237745725321345, Train accuracy: 0.8767441860465116\n",
      "Test accuracy: 0.7540603248259861\n",
      "Epoch 147, Loss: 0.012202570942022272, Train accuracy: 0.858139534883721\n",
      "Test accuracy: 0.7749419953596288\n",
      "Epoch 148, Loss: 0.00252152598417619, Train accuracy: 0.8837209302325582\n",
      "Test accuracy: 0.7911832946635731\n",
      "Epoch 149, Loss: 0.0016833042322236232, Train accuracy: 0.8534883720930233\n",
      "Test accuracy: 0.7865429234338747\n",
      "Epoch 150, Loss: 0.0009642603226058397, Train accuracy: 0.8837209302325582\n",
      "Test accuracy: 0.7865429234338747\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), stride=(stride, 1), padding=(padding, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is expected to be of shape (batch_size, in_channels, num_frames, num_nodes)\n",
    "        return self.conv(x)\n",
    "class STGCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(STGCNLayer, self).__init__()\n",
    "        self.gcn = GCNLayer(in_channels, out_channels)\n",
    "        self.tcn1 = TemporalConv(in_channels, in_channels, kernel_size, stride)\n",
    "        self.tcn2 = TemporalConv(out_channels, out_channels, kernel_size, stride)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels)  # Batch normalization\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x is expected to be of shape (batch_size, num_frames, num_nodes, in_channels)\n",
    "        batch_size, num_frames, num_nodes, in_channels = x.size()\n",
    "\n",
    "        # Apply GCN\n",
    "        x = self.gcn(x, edge_index)\n",
    "\n",
    "        # Reshape x back to (batch_size, num_frames, num_nodes, out_channels)\n",
    "        out_channels = x.size(-1)\n",
    "        x = x.view(batch_size, num_frames, num_nodes, out_channels)\n",
    "\n",
    "        # Transpose for temporal convolution: (batch_size, out_channels, num_frames, num_nodes)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Apply Temporal Convolution\n",
    "        x = self.tcn2(x)\n",
    "\n",
    "        x = self.bn(x)  # Batch normalization\n",
    "        x = self.dropout(x)  # Dropout\n",
    "\n",
    "\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Transpose back: (batch_size, num_frames, num_nodes, out_channels)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_nodes, num_layers=3, hidden_dim=64, kernel_size=3, stride=1):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(STGCNLayer(in_channels, hidden_dim, kernel_size, stride))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.layers.append(STGCNLayer(hidden_dim, hidden_dim, kernel_size, stride))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*num_nodes, num_classes)\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "\n",
    "        # Global pooling (e.g., mean pooling over time and nodes)\n",
    "        x = x.view(x.size(0), -1, self.num_nodes * x.size(-1))\n",
    "        x = x.max(axis=1)[0] # Average over time\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = STGCN(in_channels=3, num_classes=27, num_nodes=20,num_layers=3,kernel_size=5, hidden_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for file in os.listdir(\"Skeleton/\"):\n",
    "    if file.endswith(\"mat\"):\n",
    "        if \"s2\" in file or \"s4\" in file or \"s6\" in file or \"s8\" in file:\n",
    "            train.append((loadmat(\"Skeleton/\"+file)['d_skel'], file.split(\"_\")[0]))\n",
    "        else:\n",
    "            test.append((loadmat(\"Skeleton/\"+file)['d_skel'],file.split(\"_\")[0]))\n",
    "\n",
    "# Skeleton structure (joints connections) for UTD-MHAD\n",
    "skeleton_edges = [\n",
    "    (0, 1),  # head → shoulder_center\n",
    "    (1, 2),  # shoulder_center → spine\n",
    "    (2, 3),  # spine → hip_center\n",
    "\n",
    "    (1, 4),  # shoulder_center → left_shoulder\n",
    "    (4, 5),  # left_shoulder → left_elbow\n",
    "    (5, 6),  # left_elbow → left_wrist\n",
    "    (6, 7),  # left_wrist → left_hand\n",
    "\n",
    "    (1, 8),  # shoulder_center → right_shoulder\n",
    "    (8, 9),  # right_shoulder → right_elbow\n",
    "    (9, 10), # right_elbow → right_wrist\n",
    "    (10, 11),# right_wrist → right_hand\n",
    "\n",
    "    (3, 12), # hip_center → left_hip\n",
    "    (12, 13),# left_hip → left_knee\n",
    "    (13, 14),# left_knee → left_ankle\n",
    "    (14, 15),# left_ankle → left_foot\n",
    "\n",
    "    (3, 16), # hip_center → right_hip\n",
    "    (16, 17),# right_hip → right_knee\n",
    "    (17, 18),# right_knee → right_ankle\n",
    "    (18, 19) # right_ankle → right_foot\n",
    "]\n",
    "\n",
    "\n",
    "def create_graphs_from_tuple(in_tuple, use_relative_coordinates=True):\n",
    "    action = int(in_tuple[1].split('a')[1]) - 1\n",
    "    frames = in_tuple[0]  # Shape: (20, 3, num_frames)\n",
    "    \n",
    "    # Compute relative coordinates if required\n",
    "    if use_relative_coordinates:\n",
    "        central_joint_idx = 2  # Assuming the spine is the central joint\n",
    "        central_joint_coords = frames[central_joint_idx, :, :]\n",
    "        central_joint_coords = np.expand_dims(central_joint_coords, axis=0) \n",
    "        frames = frames - central_joint_coords\n",
    "\n",
    "    # Stack all frames into a single tensor of shape (num_frames, num_nodes, in_channels)\n",
    "    num_frames = frames.shape[2]\n",
    "    node_features = torch.tensor(frames.transpose(2, 0, 1), dtype=torch.float).unsqueeze(0)\n",
    "    # The transpose converts the shape to (num_frames, num_nodes, in_channels)\n",
    "    \n",
    "    # Convert edge list to tensor\n",
    "    edge_index = torch.tensor(skeleton_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create a single PyG Data object containing the entire temporal sequence\n",
    "    temporal_graph = Data(x=node_features, edge_index=edge_index, y=action)\n",
    "    \n",
    "    return temporal_graph\n",
    "\n",
    "\n",
    "\n",
    "train_graph_snapshots = [create_graphs_from_tuple(seq) for seq in train]\n",
    "test_graph_snapshots = [create_graphs_from_tuple(seq) for seq in test]\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    shuffle(train_graph_snapshots)\n",
    "    for index,graph_snapshots in enumerate(train_graph_snapshots):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        out = model(graph_snapshots.x, graph_snapshots.edge_index)  # Pass graph snapshots for one action sequence\n",
    "        labels = torch.as_tensor([graph_snapshots.y])\n",
    "        # Compute loss\n",
    "        loss = criterion(out, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    train_loss.append(np.array(losses).mean())\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for graph_snapshots in train_graph_snapshots:\n",
    "            # Forward pass through the model\n",
    "            out = model(graph_snapshots.x, graph_snapshots.edge_index)  # Pass graph snapshots for one action sequence\n",
    "            label = torch.as_tensor([graph_snapshots.y])\n",
    "            pred = torch.argmax(out)\n",
    "            if pred.item() == label.item():\n",
    "                correct +=1\n",
    "            total +=1\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.array(losses).mean()}, Train accuracy: {correct/total}')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for graph_snapshots in test_graph_snapshots:\n",
    "            # Forward pass through the model\n",
    "            out = model(graph_snapshots.x, graph_snapshots.edge_index)  # Pass graph snapshots for one action sequence\n",
    "            label = torch.as_tensor([graph_snapshots.y])\n",
    "            # Compute loss\n",
    "            pred = torch.argmax(out)\n",
    "            if pred.item() == label.item():\n",
    "                correct +=1\n",
    "            total +=1\n",
    "        print(f'Test accuracy: {correct/total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a constructed skeleton graph from frame #10 in a sequence of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_frames = train_graph_snapshots[1].x.shape[1]\n",
    "node_features = train_graph_snapshots[0].x.squeeze().permute(1,2,0)\n",
    "\n",
    "\n",
    "# Visualize the first frame as an example\n",
    "frame_num = 10\n",
    "joint_positions = node_features[:, :, frame_num]\n",
    "\n",
    "G = nx.Graph()\n",
    "for i, feature in enumerate(joint_positions.squeeze()):\n",
    "    G.add_node(i, pos=feature)\n",
    "G.add_edges_from(skeleton_edges)\n",
    "\n",
    "pos = {i: [x[0].item(),x[1].item()] for i, x in enumerate(joint_positions.squeeze())}\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(4, 4))\n",
    "nx.draw(G, pos=pos, node_color='red',edge_color=\"blue\",width=2,node_size=50)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
