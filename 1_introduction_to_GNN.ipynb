{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for graph data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know what is a graph. But how we can apply *machine learning* techniques to\n",
    "graph data? More specifically, how we can use break-throughts in *neural networks* for graph data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive approach #1\n",
    "\n",
    "We will start with the simplest GNN architecture, one where we learn new embeddings for graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph. \n",
    "This GNN uses a separate multilayer perceptron on each component of a graph; we call this a GNN layer. For each node vector, we apply the MLP and get back a learned node-vector. We do the same for each edge, learning a per-edge embedding, and also for the global-context vector, learning a single embedding for the entire graph.\n",
    "As is common with neural networks modules or layers, we can stack these GNN layers together.\n",
    "\n",
    "<center><img src=\"images/graph_attributes.png\" width=500></center>\n",
    "<center><img src=\"images/naive_1.png\" width=500></center>\n",
    "<small>A single layer of a simple GNN. A graph is the input, and each component (V,E,U) gets updated by a MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of a GNN model.</small>\n",
    "<i><center><small>images from https://distill.pub/2021/gnn-intro/</small></center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example in Python: *Classifying nodes* with vanilla neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/cora.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 2708\n",
      "number of features: 1433\n",
      "number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "\n",
    "# Load the CORA dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "# Access the first graph object\n",
    "data = dataset[0]\n",
    "print(f\"number of nodes: {data.x.shape[0]}\")\n",
    "print(f\"number of features: {data.x.shape[1]}\")\n",
    "print(f\"number of classes: {torch.unique(data.y).size()[0]}\")\n",
    "\n",
    "# The data object contains train_mask, val_mask, and test_mask\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.9247, Train Accuracy: 0.1214, Test Accuracy: 0.2130\n",
      "Epoch: 10, Loss: 1.5306, Train Accuracy: 0.9929, Test Accuracy: 0.4990\n",
      "Epoch: 20, Loss: 1.3995, Train Accuracy: 1.0000, Test Accuracy: 0.5070\n",
      "Epoch: 30, Loss: 1.4643, Train Accuracy: 1.0000, Test Accuracy: 0.4810\n",
      "Epoch: 40, Loss: 1.4934, Train Accuracy: 1.0000, Test Accuracy: 0.4950\n",
      "Epoch: 50, Loss: 1.4664, Train Accuracy: 1.0000, Test Accuracy: 0.5050\n",
      "Epoch: 60, Loss: 1.4076, Train Accuracy: 1.0000, Test Accuracy: 0.5210\n",
      "Epoch: 70, Loss: 1.3570, Train Accuracy: 1.0000, Test Accuracy: 0.5230\n",
      "Epoch: 80, Loss: 1.3292, Train Accuracy: 1.0000, Test Accuracy: 0.5260\n",
      "Epoch: 90, Loss: 1.3164, Train Accuracy: 1.0000, Test Accuracy: 0.5280\n",
      "Epoch: 100, Loss: 1.3045, Train Accuracy: 1.0000, Test Accuracy: 0.5290\n",
      "Epoch: 110, Loss: 1.2928, Train Accuracy: 1.0000, Test Accuracy: 0.5280\n",
      "Epoch: 120, Loss: 1.2843, Train Accuracy: 1.0000, Test Accuracy: 0.5320\n",
      "Epoch: 130, Loss: 1.2776, Train Accuracy: 1.0000, Test Accuracy: 0.5360\n",
      "Epoch: 140, Loss: 1.2721, Train Accuracy: 1.0000, Test Accuracy: 0.5420\n",
      "Epoch: 150, Loss: 1.2668, Train Accuracy: 1.0000, Test Accuracy: 0.5450\n",
      "Epoch: 160, Loss: 1.2620, Train Accuracy: 1.0000, Test Accuracy: 0.5460\n",
      "Epoch: 170, Loss: 1.2581, Train Accuracy: 1.0000, Test Accuracy: 0.5500\n",
      "Epoch: 180, Loss: 1.2543, Train Accuracy: 1.0000, Test Accuracy: 0.5500\n",
      "Epoch: 190, Loss: 1.2513, Train Accuracy: 1.0000, Test Accuracy: 0.5560\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_h)\n",
    "        self.linear2 = nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "mlp = MLP(dataset.num_features, 16, dataset.num_classes)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_data = TensorDataset(torch.as_tensor(data.x[train_mask,:]),torch.as_tensor(data.y[train_mask]))\n",
    "test_data = TensorDataset(torch.as_tensor(data.x[test_mask,:]),torch.as_tensor(data.y[test_mask]))\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    prediction = mlp(train_data.tensors[0])\n",
    "    loss = loss_fn(prediction,train_data.tensors[1])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_acc = accuracy(torch.argmax(prediction, dim=1),train_data.tensors[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = mlp(test_data.tensors[0])\n",
    "        loss = loss_fn(prediction,test_data.tensors[1])\n",
    "        test_acc = accuracy(torch.argmax(prediction, dim=1),test_data.tensors[1])\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approach #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DeepWalk* is a groundbreaking method that uses techniques from language modeling to learn how to represent nodes in a network. It focuses on the local connections and relationships between nodes to create these representations.\n",
    "\n",
    "\n",
    "DeepWalk works in two main steps:\n",
    "\n",
    "- Random Walks: It explores the network by randomly moving from one node to another, capturing the local structure and neighborhood relationships.\n",
    "- SkipGram: It then uses a technique called SkipGram to learn node embeddings, which are representations that incorporate the patterns and structures found in the first step.\n",
    "\n",
    "### Random Walk:\n",
    "\n",
    "In this step, the goal is to find neighborhoods in the network. Here's how it works:\n",
    "\n",
    "Generate Random Walks: We start at each node and create a set number (k) of random paths, each with a fixed length (l). After this process, we have k sequences of nodes, each sequence being l nodes long. The intuition is that nodes next to each other in these paths are similar and should have similar embeddings. Nodes that frequently appear together in these random walks are considered similar, as edges in a network usually connect similar or interacting nodes.\n",
    "\n",
    "### SkipGram:\n",
    "\n",
    "The SkipGram algorithm is a well-known method used to learn word embeddings. It was introduced by Mikolov and his team in their famous word2vec paper.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Context Windows: Given a text and a window size, SkipGram looks at words that appear close to each other within these windows (also called contexts).\n",
    "Learning Embeddings: The algorithm aims to make the embeddings (representations) of words that occur in the same context similar to each other.\n",
    "The idea behind this is simple: words that appear together often usually have similar meanings, so their embeddings should be close to each other.\n",
    "\n",
    "For an intuitive tutorial on Word2Vec, See https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ \n",
    "\n",
    "\n",
    "To use SkipGram for networks:\n",
    "\n",
    "- Context in Networks: Treat each random walk as a context, similar to how word windows work in text.\n",
    "- Starting Point: Begin with random vectors for each node in the network.\n",
    "- Embedding Updates: Iterate over the random walks and adjust the node embeddings. The goal is to make nodes that appear together in these walks have similar embeddings. This adjustment is done using gradient descent and the softmax function to ensure that the embeddings reflect the node similarities. This is done using a simple neural network.\n",
    "\n",
    "<center><img src=\"images/deepwalk.jpg\" width=400></center>\n",
    "<center><small>image from https://www.geeksforgeeks.org/deepwalk-algorithm/</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of DeepWalk in Python:\n",
    "\n",
    "To use DeepWalk for node classification on the Cora dataset, we'll follow these steps:\n",
    "\n",
    "- Load the Cora dataset.\n",
    "- Generate random walks to capture the local structure of the graph.\n",
    "- Learn node embeddings using the SkipGram model.\n",
    "- Train a classifier (e.g., logistic regression) using the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_walks(G, num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = [node]\n",
    "            while len(walk) < walk_length:\n",
    "                cur = walk[-1]\n",
    "                neighbors = list(G.neighbors(cur))\n",
    "                if neighbors:\n",
    "                    walk.append(random.choice(neighbors))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DeepWalk is: 0.7060\n"
     ]
    }
   ],
   "source": [
    "from nodevectors import Node2Vec\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# The data object contains train_mask, val_mask, and test_mask\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# Convert to NetworkX graph for random walk generation\n",
    "G = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
    "# Parameters\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "# Generate random walks\n",
    "walks = generate_random_walks(G, num_walks, walk_length)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Convert walks to strings for gensim\n",
    "walks_str = [[str(node) for node in walk] for walk in walks]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(walks_str, vector_size=128, window=10, min_count=0, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# Extract embeddings\n",
    "node_embeddings = {int(node): model.wv[str(node)] for node in G.nodes()}\n",
    "\n",
    "# Prepare data for classification\n",
    "X_train = np.array([node_embeddings[node.item()] for node in torch.where(train_mask)[0]])\n",
    "y_train = np.array(data.y[train_mask])\n",
    "\n",
    "# Train logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "X_test = np.array([node_embeddings[node.item()] for node in torch.where(test_mask)[0]])\n",
    "y_test = np.array(data.y[test_mask])\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy of DeepWalk is: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vanilla neural network (NN) focuses solely on the attributes of nodes, disregarding the underlying structure of the graph. This means it cannot leverage the relationships and connections between nodes, which are often crucial for tasks like node classification. On the other hand, DeepWalk excels in capturing the structural information of the graph by generating node embeddings based on random walks, but it completely ignores node attributes. These two approaches are at opposite extremes: one neglects structure and the other overlooks attributes. A more integrated approach is needed to combine both node features and graph structure, providing a more comprehensive representation that leverages the strengths of both attributes and connectivity for more effective learning on graph-structured data.\n",
    "\n",
    "<center><img src=\"images/Two-extremes.png\" width=500></center>\n",
    "\n",
    "So how to consider both attributes and connectivity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network: Combining Attributes and Connectivity\n",
    "\n",
    "Suppose our graph neural network starts with a graph and a set of feature vectors, where each node in the graph has its own feature vector.\n",
    "<center><img src=\"images/gcn_intro.png\" width=\"300\"></center>\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ represent the feature matrix, where $n$ denotes the number of nodes and $d$ is the feature dimension. To apply a single layer of a neural network to these features, a straightforward approach generates new embedding features as:\n",
    "\n",
    "\\begin{equation*}\n",
    " H^{(1)} = \\sigma(XW), \n",
    "\\end{equation*}\n",
    "\n",
    "where $\\sigma$ is a point-wise nonlinear function (such as ReLU), and $W \\in \\mathbb{R}^{d \\times b}$ is a weight matrix learned during training. However, this equation overlooks the local neighborhood of each node. To incorporate connectivity, we modify the approach as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    " H^{(1)} = \\sigma(AXW),\n",
    "\\end{equation*}\n",
    "\n",
    "where $A \\in \\mathbb{R}^{n \\times n}$ is the adjacency matrix. This concept is illustrated in the figure below:\n",
    "\n",
    "<center><img src=\"images/GCN_naive.png\" width=\"500\"></center>\n",
    "\n",
    "Notice how the embedding of node A now depends on its *neighbors* B and C. The adjacency matrix contains the connections between every node in the graph. Multiplying the input matrix by this adjacency matrix aggregates the neighboring node features.\n",
    "\n",
    "However, there are still two problems:\n",
    "\n",
    "- **Problem #1:** The new embedding for each node does not consider the feature of the node itself.\n",
    "    - **Solution:** Add an identity matrix $I$ to $A$ to create a new adjacency matrix $\\tilde{A}$. This addition includes self-loops, ensuring the central node is also considered:\n",
    "\n",
    "    \\begin{equation*} \\tilde{A} = A + I \\end{equation*}\n",
    "\n",
    "- **Problem #2:** If node $A$ has 1,000 neighbors and node $B$ has only 1, the embedding $H_A$ will have much larger values than $H_B$. This disparity makes meaningful comparisons between embeddings difficult.\n",
    "    - **Solution:** Normalize the embeddings by the number of neighbors. \n",
    "\n",
    "    \\begin{equation*} H[i,:] = \\frac{1}{\\text{deg}(i)} \\sum_{j \\in \\mathcal{N}_i} X[j,:]W \\end{equation*}\n",
    "\n",
    "Let $D$ be the diagonal degree matrix, where $D[i,i] = \\sum_{j=1}^n \\tilde{A}[i,j]$. The inverse of this matrix $D^{-1}$ provides the normalization coefficients.\n",
    "\n",
    "To incorporate normalization, we consider:\n",
    "\n",
    "- $D^{-1} \\tilde{A} XW$ will normalize each row of features.\n",
    "- $\\tilde{A} D^{-1} XW$ will normalize each column of features.\n",
    "\n",
    "Combining both, we derive the normalization formula:\n",
    "\n",
    "\\begin{equation*} D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}} XW \\end{equation*}\n",
    "\n",
    "This defines a layer of a *Graph Convolutional Network (GCN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN: More formal explanation\n",
    "\n",
    "*[Note: borrowed heavily from excellent blog https://mbernste.github.io/posts/gcn/]*\n",
    "\n",
    "A GCN consists of several graph convolutional layers that progressively change the feature vectors at each node. The result is a graph where each node has a new output vector. These output vectors can have different dimensions than the original input vectors.\n",
    "\n",
    "**Message Passing** is the core concept of GCN layers. The convolutional layer $l$ in GCN uses the node vectors from the previous layer $H^{(l-1)}$ (the input feature vectors for the first layer, that is, $H^{0}=X$) and creates new output vectors for each node. It does this by combining the vectors from each node's neighbors, as shown below:\n",
    "\n",
    "<center><img src=\"images/GCN_overview.png\" width=\"500\"></center>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<i>node $A$’s vector, that is, $x_A$ is pooled/aggregated with the vectors of its neighbors, $x_B$ and $x_C$. This pooled vector is then transformed/updated to form node $A$’s vector in the next layer. This same procedure is carried out over every node.</i></div>\n",
    "\n",
    "<center><small>image from https://mbernste.github.io/posts/gcn/ </small></center>\n",
    "\n",
    "The intuitive concept of message-passing is shown in the following gif. Each node sends its vector to its neighbors to help update their vectors. Essentially, the \"message\" from each node is its own vector.\n",
    "\n",
    "<center><img src=\"images/message_passing.gif\"></center>\n",
    "<center><small>gif from https://medium.com/@ashes192000/graph-neural-networks-part-4-e54251d0256d</small></center>\n",
    "\n",
    "The graph convolutional layer can be seen as a function that takes two inputs and produces a matrix with the updated vectors for each node:\n",
    "\n",
    "\\begin{equation*} H^{(l)}=\\sigma \\big(D^{-\\frac{1}{2}} (A+I) D^{-\\frac{1}{2}} H^{(l-1)}W\\big) \\end{equation*}\n",
    "\n",
    "<small>*Note that The matrix $D$ is the degree matrix of $A+I$.*\n",
    "\n",
    "A diagram showing multiple stacked graph convolutional layers is shown below:\n",
    "\n",
    "<center><img src=\"images/GCN_stacked.png\" width=\"400\"></center>\n",
    "<center><small> image from https://mbernste.github.io/posts/gcn/</small></center>\n",
    "\n",
    "## Okay... but where is the convolution?\n",
    "\n",
    "In fact, a Graph Convolutional Neural Network (GCN) performs operations similar to those in Convolutional Neural Networks (CNNs) used for images. In a GCN, we can think of the message passing process as moving a filter (or kernel) over each node in the graph. When the filter is on a node, it gathers and combines data from nearby nodes to create the output for that node.\"\n",
    "\n",
    "<img src=\"images/CNN_filter.png\" style=\"width: 45%; display: inline-block;\">\n",
    "<img src=\"images/GCN_filter.png\" style=\"width: 45%; display: inline-block;\">\n",
    "<center>convolution in CNN vs convolution in GCN</center>\n",
    "<center><small>images from https://mbernste.github.io/posts/gcn/</small></center>\n",
    "\n",
    "Very similar to CNN, for GCNs, a filter is passed over each node and the values of the **neighboring nodes** are combined to form the output value at the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN in python using Geometric library\n",
    "\n",
    "This code implements and trains a Graph Convolutional Network (GCN) using the PyTorch Geometric library on the Cora citation dataset. The GCN model consists of two graph convolutional layers that process the graph-structured data to classify nodes into different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.9465, Train Accuracy: 0.1500, Test Accuracy: 0.0980\n",
      "Epoch: 10, Loss: 0.6023, Train Accuracy: 0.9857, Test Accuracy: 0.7270\n",
      "Epoch: 20, Loss: 0.1072, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch: 30, Loss: 0.0270, Train Accuracy: 1.0000, Test Accuracy: 0.7840\n",
      "Epoch: 40, Loss: 0.0144, Train Accuracy: 1.0000, Test Accuracy: 0.7880\n",
      "Epoch: 50, Loss: 0.0128, Train Accuracy: 1.0000, Test Accuracy: 0.7920\n",
      "Epoch: 60, Loss: 0.0143, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch: 70, Loss: 0.0159, Train Accuracy: 1.0000, Test Accuracy: 0.7970\n",
      "Epoch: 80, Loss: 0.0166, Train Accuracy: 1.0000, Test Accuracy: 0.8010\n",
      "Epoch: 90, Loss: 0.0162, Train Accuracy: 1.0000, Test Accuracy: 0.8010\n",
      "Epoch: 100, Loss: 0.0154, Train Accuracy: 1.0000, Test Accuracy: 0.7980\n",
      "Epoch: 110, Loss: 0.0145, Train Accuracy: 1.0000, Test Accuracy: 0.7970\n",
      "Epoch: 120, Loss: 0.0138, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch: 130, Loss: 0.0131, Train Accuracy: 1.0000, Test Accuracy: 0.8010\n",
      "Epoch: 140, Loss: 0.0126, Train Accuracy: 1.0000, Test Accuracy: 0.8010\n",
      "Epoch: 150, Loss: 0.0121, Train Accuracy: 1.0000, Test Accuracy: 0.8040\n",
      "Epoch: 160, Loss: 0.0116, Train Accuracy: 1.0000, Test Accuracy: 0.8040\n",
      "Epoch: 170, Loss: 0.0112, Train Accuracy: 1.0000, Test Accuracy: 0.8040\n",
      "Epoch: 180, Loss: 0.0109, Train Accuracy: 1.0000, Test Accuracy: 0.8050\n",
      "Epoch: 190, Loss: 0.0105, Train Accuracy: 1.0000, Test Accuracy: 0.8060\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# Extract training, validation, and test masks from the data\n",
    "# These masks are boolean arrays indicating which nodes are used for training, validation, and testing\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# Define the GCN (Graph Convolutional Network) model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # Define the first graph convolution layer\n",
    "        self.gcn1 = GCNConv(in_dim, hidden_dim)\n",
    "        # Define the second graph convolution layer\n",
    "        self.gcn2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply the first graph convolution layer and ReLU activation\n",
    "        h = self.gcn1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        # Apply the second graph convolution layer\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h\n",
    "\n",
    "# Instantiate the GCN model\n",
    "gcn = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# Define the optimizer (Adam) and the loss function (Cross Entropy Loss)\n",
    "optimizer = optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the number of training epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# Define a function to calculate accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass: Compute predictions\n",
    "    prediction = gcn(data.x, data.edge_index)\n",
    "    \n",
    "    # Compute the loss on the training data\n",
    "    loss = loss_fn(prediction[train_mask, :], data.y[train_mask])\n",
    "    \n",
    "    # Zero the gradients, perform backpropagation, and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy(torch.argmax(prediction[train_mask, :], dim=1), data.y[train_mask])\n",
    "\n",
    "    # Evaluate on the test set without updating weights\n",
    "    with torch.no_grad():\n",
    "        test_acc = accuracy(torch.argmax(prediction[test_mask, :], dim=1), data.y[test_mask])\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- https://mbernste.github.io/posts/gcn/\n",
    "- https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72\n",
    "- LABONNE, Maxime. \"Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch.\" Packt Publishing Ltd, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
