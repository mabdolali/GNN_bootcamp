{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need\n",
    "\n",
    "The paper \"Attention Is All You Need,\" published by Vaswani et al. in 2017, introduces the Transformer model, a novel architecture designed for natural language processing tasks like translation. This model replaces the traditional sequence-based approaches, such as RNNs and LSTMs, with a purely attention-based mechanism.\n",
    "\n",
    "**Self-Attention Mechanism**: The core idea is to compute the relationships between different words in a sentence, regardless of their positions. Each word (or token) attends to all other words to determine their significance.\n",
    "\n",
    "Self-attention operates by transforming the input sequence into three vectors: query, key, and value. These vectors are obtained through linear transformations of the input. The attention mechanism calculates a weighted sum of the values based on the similarity between the query and key vectors. The resulting weighted sum, along with the original input, is then passed through a feed-forward neural network to produce the final output. This process allows the model to focus on relevant information and capture long-range dependencies.\n",
    "<center><img src=\"images\\Self_attention.png\" width=\"600\"></center>\n",
    "\n",
    "in general we can formulate that like:\n",
    "\n",
    "$$\n",
    "Attention(Q,K,V) = softmax(\\frac{QK^{t}}{\\sqrt{d_{model}}} , axis=1)V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention has several benefits that make it important in machine learning and artificial intelligence:\n",
    "\n",
    "- Long-range dependencies: Self-attention allows the model to capture relationships between distant elements in a sequence, enabling it to understand complex patterns and dependencies.\n",
    "\n",
    "- Contextual understanding: By attending to different parts of the input sequence, self-attention helps the model understand the context and assign appropriate weights to each element based on its relevance.\n",
    "\n",
    "- Parallel computation: Self-attention can be computed in parallel for each element in the sequence, making it computationally efficient and scalable for large datasets.\n",
    "\n",
    "**Positional Encoding**: Since the Transformer doesn't process words sequentially, positional encodings are added to input embeddings to retain information about the position of each word in a sentence.\n",
    "\n",
    "**Multi-Head Attention**: Multiple attention heads are used to capture different aspects of the relationships between words, allowing the model to focus on different parts of the sentence simultaneously.\n",
    "\n",
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).\n",
    "\n",
    "<center><img src=\"images\\multi-head-attention.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed-Forward Networks**: each layer has a feed-forward network block: two linear layers with RELU non-linearity between them in order to process this new information\n",
    "$$\n",
    "FFN(x) = W_2max(0,W_1x+b_1) + b_2\n",
    "$$\n",
    "<center><img src=\"images\\Feedforward.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder-Decoder Structure**: The Transformer consists of an encoder to process the input sequence and a decoder to generate the output sequence. Both the encoder and decoder have multiple layers of self-attention and feed-forward networks but for our purpose we focus on encoder.\n",
    "\n",
    "**Layer Normalization**: We normalize vector representation of each token,LayerNorm has trainable parameters,**scale** and **bias**, which are used after normalization to rescale layers's outputs\n",
    "\n",
    "**The residual connection**: establishes a direct path for the gradient, ensuring that vectors are updated rather than entirely replaced by the attention layers. This helps with gradient flow during training.\n",
    "\n",
    "<center><img src=\"images\\Transformer_encoder.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Transformers\n",
    "The goal is to generalize Transformers to arbitrary graphs or better say, design a Graph Transformer.Extend key principles behind Transformer’s \n",
    "success to graph structured datasets.But we have problems\n",
    "\n",
    "**Graph Sparsity**: NLP Transformers consider fully connected graph and this choice can be justified for two reasons:\n",
    "- Difficult to find meaningful sparse connections between words, i.e. absence of sparse structure.\n",
    "- NLP Transformer sentences often contain less than tens or hundreds of words, hence scalable to consider full attention.\n",
    "\n",
    "But, graphs have arbitrary connectivity structure,\n",
    "- And full attention is not feasible (millions/billions of nodes)\n",
    "\n",
    "GNNs are state of the art on several application domains since they consider sparse structure into account while learning feature representations.\n",
    "\n",
    "<center><img src=\"images\\Local_vs_global_attention.png\" width=\"300\"></center>\n",
    "\n",
    "**Positional Encodings (PE)**: Like Transformers its natural to use a mechanism to encode node positions in Graph Transformers,We use Laplacian Eigenvectors corresponding to a node, as the node positional encoding – namely Laplacian PE (λ) , which generalize the sinusoidal PE(~) used in NLP Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Architecture for Generalization\n",
    "we have two architectures one of them includes edge features as well\n",
    "\n",
    "<center><img src=\"images\\Graph_Transformer.png\" width=\"700\"></center>\n",
    "\n",
    "Compared to the original transforme the highlights of the proposed architecture are:\n",
    "1. The attention mechanism is a function of neighborhood connectivity for each node.\n",
    "2. Positional encoding is represented by Laplacian PE – k smallest non-trivial eigenvectors of a node.\n",
    "3. The layer normalization is replaced by a batch normalization layer.\n",
    "4. The architecture is extended (Fig) to have edge representation, which can be critical to tasks with rich information on the edges.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
