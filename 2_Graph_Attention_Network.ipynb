{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Network (GAT)\n",
    "\n",
    "### challenges with GCN:\n",
    "\n",
    "\n",
    "GCNs work by aggregating information from a node’s neighbors to update its feature representation. One common approach is to use a normalization factor based on the degrees of the nodes, that is, $\\frac{1}{\\sqrt{deg(i)}\\sqrt{deg(j)}}$. This means nodes with fewer neighbors are given more weight. The idea is to balance the influence of each node's neighbors so that nodes with many neighbors don't dominate the feature updates. As a result, nodes with fewer neighbors are considered more important.\n",
    "\n",
    "The normalization based solely on node degrees can be limiting. It doesn't take into account the actual features of the nodes. Therefore, important feature information might be overlooked, and nodes with low degrees might be given undue importance regardless of their actual relevance to the task.\n",
    "\n",
    "GATs address these challenges by introducing attention mechanisms into the aggregation process. Instead of using a fixed normalization factor, GATs learn dynamic weighting factors for each neighbor, allowing the network to focus on the most relevant nodes and features.\n",
    "\n",
    "### Advantages of GATs\n",
    "\n",
    "- Feature-based Attention: GATs compute attention coefficients for each pair of connected nodes based on their features. This means the importance of a neighbor is learned from the data, considering both the node's and its neighbor's features.\n",
    "\n",
    "- Adaptive Weights: The attention mechanism allows the model to adaptively assign different weights to different neighbors, making it more flexible and powerful in capturing the underlying structure and feature significance in the graph.\n",
    "- End-to-End Training: The attention coefficients are learned end-to-end with the rest of the model parameters, allowing the GAT to optimize these weights for the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph attentional layer\n",
    "\n",
    "A key feature of GATs is their ability to calculate *attention scores* through a mechanism known as self-attention. This involves comparing node features to each other to determine the importance of neighboring nodes, allowing the model to focus on the most relevant parts of the graph.\n",
    "\n",
    "In traditional graph neural networks, the aggregation of information from neighboring nodes often treats all neighbors with equal importance. However, GATs introduce a more nuanced approach by assigning different attention scores to different neighbors, enabling the network to learn which connections are more significant for the task at hand.\n",
    "\n",
    "We will delve into the detailed process of calculating these attention scores in four steps:\n",
    "\n",
    "- Linear transformation\n",
    "- Computing self-attention scores\n",
    "- Softmax normalization\n",
    "- Multi-head attention\n",
    "\n",
    "### Linear transformation\n",
    "\n",
    "The input to our layer is a set of node features, $h = \\{h_1, h_2, ..., h_n\\}, h_i \\in \\mathbb{R}^F$ , where $n$ is the\n",
    "number of nodes, and $F$ is the number of features in each node. The layer produces a new set of node features (of potentially different cardinality $F'$), $h' = \\{h'_1, h'_2, ..., h'_n\\}, h'_i \\in \\mathbb{R}^{F'}$, as its output.\n",
    "In order to obtain sufficient expressive power to transform the input features into higher-level features, at least one learnable linear transformation is required. To that end, as an initial step, a shared linear transformation, parametrized by a weight matrix, $W \\in \\mathbb{R}^{F' \\times F}$ , is applied to every node.\n",
    "\n",
    "<center><img src=\"images/GAT1.png\" width=600></center>\n",
    "<center><small>image from https://epichka.com/blog/2023/gat-paper-explained/</small></center>\n",
    "\n",
    "### Computing self-attention scores\n",
    "\n",
    "We then perform self-attention on the nodes— *a shared attentional mechanism $f: \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}$* to computes attention coefficients:\n",
    "\\begin{equation*}\n",
    "\\alpha_{ij} = f(Wh_i, Wh_j)\n",
    "\\end{equation*}\n",
    "that indicate the importance of node $j$’s features to node $i$. We inject the graph structure into the mechanism by performing **masked attention**— we only compute $\\alpha_{ij}$ for nodes $j \\in \\mathcal{N}_i$, where $\\mathcal{N}_i$ is some neighborhood of node i in the graph.\n",
    "\n",
    "The attention mechanism $f$ is a single-layer feedforward neural network, parametrized by a weight vector $\\alpha \\in \\mathbb{R}^{2F'}$, and applying the LeakyReLU nonlinearity.\n",
    "\\begin{equation*}\n",
    "\\alpha_{ij} = exp\\left(LeakyReLU\\left(a^T [Wh_i||Wh_j]\\right)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "### Softmax normalization\n",
    "\n",
    "To make coefficients easily comparable across different nodes, we normalize them across all choices of $j$ using the softmax function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{ij}=\\frac{exp(LeakyReLU\\left(a^T [Wh_i||Wh_j]\\right))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU\\left(a^T [Wh_i||Wh_j]\\right))}\n",
    "\\end{equation}\n",
    "<center><img src=\"images/GAT2.png\" width=800></center>\n",
    "<center><small>image from https://epichka.com/blog/2023/gat-paper-explained/</small></center>\n",
    "\n",
    "\n",
    "Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for every node:\n",
    "\n",
    "<center><img src=\"images/GAT3.png\" width=800></center>\n",
    "<center><small>image from https://epichka.com/blog/2023/gat-paper-explained/</small></center>\n",
    "\n",
    "### Multi-head attention\n",
    "\n",
    "To stabilize the learning process of self-attention, we extend this mechanism to employ multi-head attention to be beneficial.  Specifically, $K$ independent attention mechanisms execute the transformation of Equation (1), and then their features are\n",
    "concatenated (or averaged), resulting in the following output feature representation:\n",
    "\n",
    "<center><img src=\"images/multihead_attention.png\" width=400></center>\n",
    "<center><small>image from https://arxiv.org/pdf/1710.10903</small></center>\n",
    "\n",
    "<center><img src=\"images/multihead.png\" width=600></center>\n",
    "<center><small>Illustration of multi-head attention mechanism used in the GAT model. Here, the number of attention is set as K = 3. The aggregated features from each head are concatenated or averaged to obtain a higher representation for each node. The average operation is only used in the output layer. The self-connection is not considered</small></center>\n",
    "<center><small>image from https://link.springer.com/article/10.1007/s00138-021-01251-0</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, Graph Attention Networks (GAT) are implemented in PyTorch Geometric using a slightly different approach for efficiency. Instead of directly masking the attention scores based on the graph's adjacency matrix, it is more efficient to initially compute the attention coefficients $e_{ij}$ for **all possible pairs of nodes** and then select only those that correspond to the existing edges. This approach leverages the sparse nature of real-world graphs and ensures computational efficiency.\n",
    "\n",
    "so at first, we compute attention scores for all pairs of nodes as follows:\n",
    "<center><img src=\"images/GAT_pytorch.png\" width=600></center>\n",
    "<center><small>image from https://epichka.com/blog/2023/gat-paper-explained/</small></center>\n",
    "\n",
    "To ensure that attention is only applied to existing edges, we use the adjacency matrix of the graph. This matrix is used to mask the non-existent edges by assigning a large negative value (such as $-\\infty$) to $e_{ij}$ for pairs of nodes that are not connected. This effectively zeroes out their corresponding attention weights after the softmax operation. This is illustrated in the following figure:\n",
    "\n",
    "<center><img src=\"images/GAT_pytorch2.png\" width=600></center>\n",
    "<center><small>image from https://epichka.com/blog/2023/gat-paper-explained/</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT in python using Geometric library\n",
    "\n",
    "The following code demonstrates how to implement a Graph Attention Network (GAT) using PyTorch Geometric with the Cora dataset. It involves defining a GAT model with two graph attention layers, setting up the dataset and data masks, and training the model over a specified number of epochs. The training loop includes computing predictions, calculating the loss, performing backpropagation, and evaluating the model's accuracy on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.9455, Train Accuracy: 0.2857, Test Accuracy: 0.1800\n",
      "Epoch: 10, Loss: 1.8028, Train Accuracy: 0.7929, Test Accuracy: 0.5110\n",
      "Epoch: 20, Loss: 1.6520, Train Accuracy: 0.9357, Test Accuracy: 0.8050\n",
      "Epoch: 30, Loss: 1.4082, Train Accuracy: 0.9214, Test Accuracy: 0.7420\n",
      "Epoch: 40, Loss: 1.2132, Train Accuracy: 0.9571, Test Accuracy: 0.8070\n",
      "Epoch: 50, Loss: 1.0593, Train Accuracy: 0.9571, Test Accuracy: 0.8180\n",
      "Epoch: 60, Loss: 0.9141, Train Accuracy: 0.9643, Test Accuracy: 0.7970\n",
      "Epoch: 70, Loss: 0.9031, Train Accuracy: 0.9786, Test Accuracy: 0.8230\n",
      "Epoch: 80, Loss: 0.8355, Train Accuracy: 0.9857, Test Accuracy: 0.8090\n",
      "Epoch: 90, Loss: 0.7935, Train Accuracy: 0.9857, Test Accuracy: 0.7900\n",
      "Epoch: 100, Loss: 0.6930, Train Accuracy: 0.9857, Test Accuracy: 0.8190\n",
      "Epoch: 110, Loss: 0.7375, Train Accuracy: 0.9929, Test Accuracy: 0.8210\n",
      "Epoch: 120, Loss: 0.7936, Train Accuracy: 0.9786, Test Accuracy: 0.7940\n",
      "Epoch: 130, Loss: 0.6390, Train Accuracy: 0.9929, Test Accuracy: 0.8020\n",
      "Epoch: 140, Loss: 0.6337, Train Accuracy: 0.9857, Test Accuracy: 0.8190\n",
      "Epoch: 150, Loss: 0.5687, Train Accuracy: 1.0000, Test Accuracy: 0.8340\n",
      "Epoch: 160, Loss: 0.7414, Train Accuracy: 1.0000, Test Accuracy: 0.8190\n",
      "Epoch: 170, Loss: 0.6011, Train Accuracy: 0.9929, Test Accuracy: 0.8060\n",
      "Epoch: 180, Loss: 0.6132, Train Accuracy: 0.9857, Test Accuracy: 0.8050\n",
      "Epoch: 190, Loss: 0.6076, Train Accuracy: 1.0000, Test Accuracy: 0.8190\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\",transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# Extract training, validation, and test masks from the data\n",
    "# These masks are boolean arrays indicating which nodes are used for training, validation, and testing\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# Define the GCN (Graph Convolutional Network) model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, heads=8):\n",
    "        super().__init__()\n",
    "        # Define the first graph convolution layer\n",
    "        self.gat1 = GATv2Conv(in_dim, hidden_dim, heads=8, dropout=0.6)\n",
    "        # Define the second graph convolution layer\n",
    "        self.gat2 = GATv2Conv(hidden_dim*heads, out_dim, heads=1, concat=False, dropout=0.6)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training)\n",
    "        # Apply the first GAT and eLU activation\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.6, training=self.training)\n",
    "        # Apply the second GAT layer\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return h \n",
    "\n",
    "# Instantiate the GAT model\n",
    "gat = GAT(dataset.num_features, 32, dataset.num_classes)\n",
    "\n",
    "# Define the optimizer (Adam) and the loss function (Cross Entropy Loss)\n",
    "optimizer = optim.Adam(gat.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the number of training epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# Define a function to calculate accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    gat.train()\n",
    "    # Forward pass: Compute predictions\n",
    "    prediction = gat(data.x, data.edge_index)\n",
    "    \n",
    "    # Compute the loss on the training data\n",
    "    loss = loss_fn(prediction[train_mask, :], data.y[train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    # Zero the gradients, perform backpropagation, and update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        # Evaluate on the test set without updating weights\n",
    "        with torch.no_grad():\n",
    "            gat.eval()\n",
    "            prediction = gat(data.x, data.edge_index)\n",
    "            # Calculate training accuracy\n",
    "            train_acc = accuracy(torch.argmax(prediction[train_mask, :], dim=1), data.y[train_mask])\n",
    "            test_acc = accuracy(torch.argmax(prediction[test_mask, :], dim=1), data.y[test_mask])\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bero310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
