{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Geometric (PyG) Tutorial\n",
    "\n",
    "Borrowed from: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyG (PyTorch Geometric) is a library built upon  PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n",
    "\n",
    "Installation:\n",
    "\n",
    ">pip install torch_geometric\n",
    "\n",
    "If you want to utilize the full set of features from PyG, there exists several additional libraries you may want to install:\n",
    "\n",
    "- pyg-lib\n",
    "- torch-scatter\n",
    "- torch-sparse\n",
    "- torch-cluster\n",
    "- torch-spline-conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in PyG\n",
    "\n",
    "A graph is used to model pairwise relations (edges) between objects (nodes). A single graph in PyG is described by an instance of `torch_geometric.data.Data`, which holds the following attributes by default (although none of these attributes are strictly required):\n",
    "\n",
    "- data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "- data.edge_index: Graph connectivity with shape [2, num_edges] and type torch.long\n",
    "\n",
    "- data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "- data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "#Note: the elements in edge_index only hold indices in the range { 0, ..., num_nodes - 1}\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Basic attributes of Data object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "1\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.num_node_features)\n",
    "print(data.has_isolated_nodes())\n",
    "print(data.has_self_loops())\n",
    "print(data.is_directed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Benchmark Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyG provides access to a wide range of popular benchmark datasets, including all Planetoid datasets (Cora, Citeseer, Pubmed), graph classification datasets from TUDatasets (along with their cleaned versions), and several 3D mesh/point cloud datasets like FAUST, ModelNet10/40, and ShapeNet.\n",
    "\n",
    "Loading a dataset is simple—PyG automatically downloads and processes the raw data into the standardized Data format.\n",
    "\n",
    "Let’s download Cora, the standard benchmark dataset for semi-supervised graph node classification. It consists of scientific publications, where:\n",
    "- Nodes represent individual research papers.\n",
    "- Edges represent citation relationships between these papers.\n",
    "- Each node is described by a feature vector based on the presence of certain words in the paper's abstract.\n",
    "- Nodes are labeled with one of seven classes, corresponding to the paper's topic (e.g., machine learning, information retrieval).\n",
    "\n",
    "The Cora dataset is commonly used to evaluate graph neural networks (GNNs) due to its graph-structured nature and relatively small size, making it an accessible starting point for experiments on citation networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load Cora dataset:\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs:  1\n",
      "Number of features:  1433\n",
      "Number of classes:  7\n",
      "Number of nodes:  2708\n",
      "Number of edges:  10556\n",
      "Is directed:  False\n",
      "# of nodes to train on:  140\n",
      "# of nodes to test on:  1000\n",
      "# of nodes to validate on:  500\n",
      "X shape:  torch.Size([2708, 1433])\n",
      "Edge shape:  torch.Size([2, 10556])\n",
      "Y shape:  torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "# number of graphs\n",
    "print(\"Number of graphs: \", len(dataset))\n",
    "# number of features\n",
    "print(\"Number of features: \", dataset.num_features)\n",
    "# number of classes\n",
    "print(\"Number of classes: \", dataset.num_classes)\n",
    "\n",
    "# select the only graph\n",
    "data = dataset[0]\n",
    "# number of nodes\n",
    "print(\"Number of nodes: \", data.num_nodes)\n",
    "# number of edges\n",
    "print(\"Number of edges: \", data.num_edges)\n",
    "# check if directed\n",
    "print(\"Is directed: \", data.is_directed())\n",
    "\n",
    "#standard splits\n",
    "# training nodes\n",
    "print(\"# of nodes to train on: \", data.train_mask.sum().item())\n",
    "# test nodes\n",
    "print(\"# of nodes to test on: \", data.test_mask.sum().item())\n",
    "# validation nodes\n",
    "print(\"# of nodes to validate on: \", data.val_mask.sum().item())\n",
    "\n",
    "print(\"X shape: \", data.x.shape)\n",
    "print(\"Edge shape: \", data.edge_index.shape)\n",
    "print(\"Y shape: \", data.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' try another one! The ENZYMES dataset in the TUDataset collection is a popular benchmark dataset used for graph classification tasks, particularly in bioinformatics. It consists of graphs representing protein structures, where the goal is to classify enzymes into one of six different enzyme classes.\n",
    "\n",
    "Here are the key details:\n",
    "\n",
    "- Graphs: The dataset contains 600 graphs, each representing the tertiary structure of a protein.\n",
    "- Nodes: Each node in a graph represents a secondary structure element (SSE), such as an α-helix or a β-sheet.\n",
    "- Edges: Edges between nodes represent spatial proximity or structural relationships between SSEs.\n",
    "- Features: Each node is labeled with a feature vector that encodes properties of the SSE, such as the type of SSE and its 3D spatial coordinates.\n",
    "- Labels: The graphs are classified into 6 enzyme classes, based on the enzyme's function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "6\n",
      "3\n",
      "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset.num_classes)\n",
    "print(dataset.num_node_features)\n",
    "print(dataset[0])\n",
    "# Note that the first graph in the dataset contains 37 nodes, each one having 3 features. \n",
    "# There are 168/2 = 84 undirected edges and the graph is assigned to exactly one class. \n",
    "# In addition, the data object is holding exactly one graph-level target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch graph processing\n",
    "\n",
    "Neural networks are typically trained using batches. PyTorch Geometric (PyG) facilitates parallel processing within a mini-batch by constructing sparse block diagonal adjacency matrices (as defined by edge_index) and concatenating feature and target matrices along the node dimension. This approach accommodates graphs with varying numbers of nodes and edges within a single batch.\n",
    "<center>\n",
    "<img src=\"images/batch.png\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2842], x=[735, 3], y=[24], batch=[735], ptr=[25])\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch in loader:\n",
    "    pass\n",
    "print(batch) #batch is a column vector which maps each node to its respective graph in the batch\n",
    "print(batch.num_graphs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Graph Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN()\n",
    "data = dataset[0]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate the model on the test nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8060\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of graph-level classification using minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PROTEINS(1113):\n",
      "======================\n",
      "Number of graphs: 1113\n",
      "Number of features: 3\n",
      "Number of classes: 2\n",
      "Number of training graphs: 890\n",
      "Number of test graphs: 223\n",
      "Epoch: 011, Train Loss: 0.0211, Train Accuracy: 0.5955, Test Loss: 0.0211, Test Accuracy: 0.5964\n",
      "Epoch: 021, Train Loss: 0.0207, Train Accuracy: 0.6191, Test Loss: 0.0209, Test Accuracy: 0.5874\n",
      "Epoch: 031, Train Loss: 0.0199, Train Accuracy: 0.6674, Test Loss: 0.0205, Test Accuracy: 0.6143\n",
      "Epoch: 041, Train Loss: 0.0197, Train Accuracy: 0.6876, Test Loss: 0.0204, Test Accuracy: 0.6502\n",
      "Epoch: 051, Train Loss: 0.0193, Train Accuracy: 0.6933, Test Loss: 0.0204, Test Accuracy: 0.6592\n",
      "Epoch: 061, Train Loss: 0.0194, Train Accuracy: 0.6933, Test Loss: 0.0204, Test Accuracy: 0.6502\n",
      "Epoch: 071, Train Loss: 0.0195, Train Accuracy: 0.6865, Test Loss: 0.0204, Test Accuracy: 0.6457\n",
      "Epoch: 081, Train Loss: 0.0193, Train Accuracy: 0.7000, Test Loss: 0.0204, Test Accuracy: 0.6457\n",
      "Epoch: 091, Train Loss: 0.0191, Train Accuracy: 0.6910, Test Loss: 0.0204, Test Accuracy: 0.6457\n",
      "Epoch: 101, Train Loss: 0.0192, Train Accuracy: 0.7067, Test Loss: 0.0204, Test Accuracy: 0.6457\n",
      "Epoch: 111, Train Loss: 0.0192, Train Accuracy: 0.6944, Test Loss: 0.0204, Test Accuracy: 0.6457\n",
      "Epoch: 121, Train Loss: 0.0190, Train Accuracy: 0.7056, Test Loss: 0.0203, Test Accuracy: 0.6502\n",
      "Epoch: 131, Train Loss: 0.0192, Train Accuracy: 0.6978, Test Loss: 0.0203, Test Accuracy: 0.6547\n",
      "Epoch: 141, Train Loss: 0.0191, Train Accuracy: 0.7034, Test Loss: 0.0203, Test Accuracy: 0.6592\n",
      "Epoch: 151, Train Loss: 0.0189, Train Accuracy: 0.7056, Test Loss: 0.0202, Test Accuracy: 0.6592\n",
      "Epoch: 161, Train Loss: 0.0189, Train Accuracy: 0.7169, Test Loss: 0.0202, Test Accuracy: 0.6637\n",
      "Epoch: 171, Train Loss: 0.0188, Train Accuracy: 0.7034, Test Loss: 0.0202, Test Accuracy: 0.6592\n",
      "Epoch: 181, Train Loss: 0.0188, Train Accuracy: 0.7079, Test Loss: 0.0202, Test Accuracy: 0.6682\n",
      "Epoch: 191, Train Loss: 0.0190, Train Accuracy: 0.7112, Test Loss: 0.0201, Test Accuracy: 0.6682\n",
      "Epoch: 201, Train Loss: 0.0190, Train Accuracy: 0.7034, Test Loss: 0.0202, Test Accuracy: 0.6592\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = TUDataset(root='/path/to/your/dataset', name='PROTEINS')\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "#splitting the dataset into training and testing\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "#creating dataloaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_1, hidden_channels_2, hidden_channels_3, hidden_channels_4):\n",
    "        \"\"\"Graph Convolutional Network with 3 layers following by a post message passing layer\"\"\"\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\n",
    "        self.conv3 = GCNConv(hidden_channels_2, hidden_channels_3)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels_3, hidden_channels_4),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels_4, dataset.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "        embedding = x\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x, embedding\n",
    "    \n",
    "model = GCN(32,64,64,32)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=8e-5, weight_decay = 1e-4)\n",
    "\n",
    "#Create empty results dictionary\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": []\n",
    "}\n",
    "\n",
    "for epoch in range(1,201):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, train_accuracy = 0, 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data.x, data.edge_index, data.batch)\n",
    "        loss = loss_fn(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += (output.argmax(dim=1) == data.y).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, test_accuracy = 0, 0\n",
    "\n",
    "        for data in test_loader:\n",
    "            output, _ = model(data.x, data.edge_index, data.batch)\n",
    "            loss = loss_fn(output, data.y)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += (output.argmax(dim=1) == data.y).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy /= len(test_loader.dataset)\n",
    "\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_accuracy\"].append(train_accuracy)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_accuracy\"].append(test_accuracy)\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bonus Tip: Loading custome datasets\n",
    "\n",
    "To create datasets that are not included with PyTorch Geometric (PyG), you can utilize the torch_geometric.data.Dataset class. This approach closely mirrors the structure and functionality of datasets found in torchvision, making it intuitive for users familiar with image data handling. It expects the following methods to be implemented in addition:\n",
    "\n",
    "- Dataset.len(): Returns the number of examples in your dataset.\n",
    "\n",
    "- Dataset.get(): Implements the logic to load a single graph.\n",
    "\n",
    "Here is a practical example of how defining a Custome Dataset can be done in PyG:\n",
    "\n",
    "The implemented `GMLDataset` class is tailored to load graph data from GML files, specifically for the `MUTAG` dataset, which includes `188` graphs stored individually in `GML` format. The class initializes with the root directory and a label file, automatically identifies the GML files, and reads the corresponding labels.\n",
    "\n",
    "Key methods include `len`, which returns the total number of graphs, and `get`, which loads a specific graph, converts it from NetworkX format to a PyTorch Geometric Data object, and assigns the appropriate label.\n",
    "\n",
    "This class serves as a blueprint for creating custom datasets in PyTorch Geometric. By inheriting from `torch_geometric.data.Dataset`, you can implement essential methods for customization based on specific datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 188\n",
      "Edge index shape: torch.Size([2, 38])\n",
      "Label: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "class GMLDataset(Dataset):\n",
    "    def __init__(self, root, label_file, transform=None, pre_transform=None):\n",
    "        super(GMLDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.root = root\n",
    "        self.label_file = label_file\n",
    "        self.graph_files = self.raw_file_names\n",
    "        self.labels = self.load_labels()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # Automatically find all GML files in the root directory\n",
    "        return [f for f in os.listdir(self.root) if f.endswith('.gml')]\n",
    "\n",
    "    def load_labels(self):\n",
    "        # Load labels from the label.txt file\n",
    "        label_path = os.path.join(self.root, self.label_file)\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = [int(line.strip()) for line in f.readlines()]\n",
    "        return labels\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        file_path = os.path.join(self.root, self.graph_files[idx])\n",
    "        # Load the graph from GML file using networkx\n",
    "        nx_graph = nx.read_gml(file_path, label=\"id\")\n",
    "        data = from_networkx(nx_graph)\n",
    "        # Assign the corresponding label to the data object\n",
    "        data.y = torch.tensor([self.labels[idx]], dtype=torch.long)\n",
    "        return data\n",
    "# Define the root directory where the GML dataset is located\n",
    "# Download the dataset from https://github.com/BorgwardtLab/P-WL/tree/master/data/MUTAG \n",
    "root_dir = 'data/MUTAG'\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = GMLDataset(root=root_dir, label_file='Labels.txt')\n",
    "\n",
    "# Print the number of graphs in the dataset\n",
    "print(\"Number of graphs:\", len(dataset))\n",
    "\n",
    "# Access the first graph data (this will trigger loading)\n",
    "graph_data = dataset[0]\n",
    "print(\"Edge index shape:\", graph_data.edge_index.shape)\n",
    "print(\"Label:\", graph_data.y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
